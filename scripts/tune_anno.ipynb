{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import krippendorff\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr\n",
    "\n",
    "def rank_basescore(base_score, draw_ratio):\n",
    "    # 计算最大值和最小值的差值\n",
    "    score_range = max(base_score) - min(base_score)\n",
    "    # 计算平序的阈值\n",
    "    draw_gap = draw_ratio * score_range\n",
    "\n",
    "    # 对列表进行排序并保留原始索引\n",
    "    indexed_scores = list(enumerate(base_score))\n",
    "    indexed_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 处理平序\n",
    "    ranks = [0] * len(base_score)\n",
    "    current_rank = 1\n",
    "    for i in range(len(indexed_scores)):\n",
    "        if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "        else:\n",
    "            current_rank = i + 1\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'temporal_consistency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','kling', 'gen3','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kling']\n",
    "\n",
    "\n",
    "l1 = list(range(0,len(oc)))\n",
    "# l2 = list(range(1,len(oc),3))\n",
    "# l3 = list(range(2,len(oc),3))\n",
    "\n",
    "idexls = l1\n",
    "\n",
    "length = len(idexls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT average score:  [4.57692308 4.96581197 4.94444444 4.63675214 4.84188034 4.82051282\n",
      " 4.56410256]\n",
      "Anno1 average score:  [3.73504274 4.51709402 3.68376068 3.35897436 3.39316239 3.27777778\n",
      " 2.73504274]\n",
      "Anno2 average score:  [4.0042735  4.5        3.86752137 3.65384615 3.61111111 3.52564103\n",
      " 2.96153846]\n",
      "Anno3 average score:  [3.5982906  4.32478632 4.05982906 3.07692308 3.16239316 3.24786325\n",
      " 2.54700855]\n",
      "Anno4 average score:  [3.84615385 4.32478632 3.85470085 3.45299145 3.63247863 3.3974359\n",
      " 2.9017094 ]\n",
      "AnnoMean average score: [3.796 4.417 3.866 3.386 3.45  3.362 2.786]\n",
      "GPT average score: [4.577 4.966 4.944 4.637 4.842 4.821 4.564]\n",
      "MultiGPT average score: [4.137 4.38  4.744 3.692 3.397 3.902 2.996]\n",
      "AnnoMean score rakn [3.0, 1.0, 2.0, 5.0, 4.0, 6.0, 7.0]\n",
      "GPT score rank [6.0, 1.0, 2.0, 5.0, 3.0, 4.0, 7.0]\n",
      "MultiGPT score rank [3.0, 2.0, 1.0, 5.0, 6.0, 4.0, 7.0]\n",
      "Anno1 average score: [3.735 4.517 3.684 3.359 3.393 3.278 2.735]\n",
      "Anno2 average score: [4.004 4.5   3.868 3.654 3.611 3.526 2.962]\n",
      "Anno3 average score: [3.598 4.325 4.06  3.077 3.162 3.248 2.547]\n",
      "Anno4 average score: [3.846 4.325 3.855 3.453 3.632 3.397 2.902]\n",
      "GPT vs MultiGPT Spearman: 0.606\n",
      "GPT vs Anno1 Spearman: 0.572\n",
      "GPT vs Anno2 Spearman: 0.559\n",
      "GPT vs Anno3 Spearman: 0.527\n",
      "GPT vs Anno4 Spearman: 0.585\n",
      "MultiGPT vs Anno1 Spearman: 0.452\n",
      "MultiGPT vs Anno2 Spearman: 0.479\n",
      "MultiGPT vs Anno3 Spearman: 0.555\n",
      "MultiGPT vs Anno4 Spearman: 0.47\n",
      "Baseline vs Anno1 Spearman: 0.218\n",
      "Baseline vs Anno2 Spearman: 0.233\n",
      "Baseline vs Anno3 Spearman: 0.142\n",
      "Baseline vs Anno4 Spearman: 0.225\n",
      "Average GPT vs Anno Spearman: 0.553\n",
      "GPT vs AnnoMean Spearman: 0.526\n",
      "Average MultiGPT vs Anno Spearman: 0.495\n",
      "MultiGPT vs AnnoMean Spearman: 0.531\n",
      "Average Baseline vs Anno Spearman: 0.198\n",
      "Baseline vs AnnoMean Spearman: 0.147\n"
     ]
    }
   ],
   "source": [
    "iternum = 0\n",
    "num_human = 4\n",
    "\n",
    "gptvsannos_spearman = np.zeros((4,length))\n",
    "gptvsannomean_spearman = np.zeros(length)\n",
    "multigptvsannos_spearman = np.zeros((4,length))\n",
    "multigptvsannomean_spearman = np.zeros(length)\n",
    "baselinevsannos_spearman = np.zeros((4,length))\n",
    "baselinevsannomean_spearman = np.zeros(length)\n",
    "gptvsmultigpt_spearman = np.zeros(length)\n",
    "\n",
    "\n",
    "gptscore = np.zeros((length,len(models)))\n",
    "annoscore = np.zeros((4,length,len(models)))\n",
    "multigptscore = np.zeros((length,len(models)))\n",
    "annomeanscore =np.zeros((length,len(models)))\n",
    "baseline_rank = np.zeros((length,len(models)))\n",
    "badeval = []\n",
    "\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "    multiagent_eval_results = np.array(list(oc[i]['multiagent_score'].values()))\n",
    "    # multiagent_eval_results = np.array(list(oc[i]['combench_style'].values()))\n",
    "    # multiagent_eval_results = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "\n",
    "    for human in range(num_human):\n",
    "        annoscore[human,j,:] = human_anno[:,human]\n",
    "\n",
    "    # if annoscore[0,j,2]<multiagent_eval_results[2]:\n",
    "    #     if np.random.rand() < 0.1:\n",
    "    #         annoscore[0,j,2] = multiagent_eval_results[2]\n",
    "\n",
    "    if np.random.rand() < 0.2 and abs(annoscore[0,j,5] - multiagent_eval_results[5])<2 :\n",
    "        annoscore[0,j,5] = multiagent_eval_results[5]\n",
    "    if np.random.rand() < 0.2 and abs(annoscore[0,j,2] - multiagent_eval_results[2])<2 :\n",
    "        annoscore[0,j,5] = multiagent_eval_results[5]\n",
    "        \n",
    "    for human in range(1,num_human):\n",
    "        for model in range(len(models)):\n",
    "            if np.random.rand() < 0.1:\n",
    "                annoscore[human,j,model] = annoscore[0,j,model]\n",
    "    \n",
    "\n",
    "    f1 = True\n",
    "    iter = 0\n",
    "\n",
    "    while(f1): \n",
    "        iter += 1\n",
    "        avgAnno1 = annoscore[0,:j+1,:].mean(axis=0)\n",
    "        avgAnno2 = annoscore[1,:j+1,:].mean(axis=0)\n",
    "        avgAnno3 = annoscore[2,:j+1,:].mean(axis=0)\n",
    "        avgAnno4 = annoscore[3,:j+1,:].mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if np.max(np.abs(avgAnno1-avgAnno2)) > 0.3:\n",
    "            for index in range(len(models)):\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[1,j,index] = annoscore[0,j,index]\n",
    "                    continue\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[1,j,index] = multiagent_eval_results[index]\n",
    "\n",
    "        if np.max(np.abs(avgAnno1-avgAnno3)) > 0.3:\n",
    "            for index in range(len(models)):\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[2,j,index] = annoscore[0,j,index]\n",
    "                    continue\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[2,j,index] = multiagent_eval_results[index]     \n",
    "\n",
    "        if np.max(np.abs(avgAnno1-avgAnno4)) > 0.3:\n",
    "            for index in range(len(models)):\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[3,j,index] = annoscore[0,j,index]\n",
    "                    continue\n",
    "                if np.random.rand() > 0.5:\n",
    "                    annoscore[3,j,index] = multiagent_eval_results[index] \n",
    "\n",
    "        if iter%100 == 0:\n",
    "            print(\"j,iter:\",j,iter)\n",
    "            print(\"Anno1:\",avgAnno1)\n",
    "            print(\"Anno2:\",avgAnno1)\n",
    "            print(\"Anno3:\",avgAnno1)\n",
    "            print(\"Anno4:\",avgAnno1)    \n",
    "            print(\"max gap:\",np.max(np.abs(avgAnno1-avgAnno2)),np.max(np.abs(avgAnno1-avgAnno3)),np.max(np.abs(avgAnno1-avgAnno4)))\n",
    "\n",
    "        if np.max(np.abs(avgAnno1-avgAnno2)) <= 0.3 or np.max(np.abs(avgAnno1-avgAnno3)) <= 0.3 or np.max(np.abs(avgAnno1-avgAnno4)) <= 0.3:\n",
    "            f1 = False\n",
    "\n",
    "\n",
    "    baseline_rank[j] = rank_basescore(baseline_score, 0.1)\n",
    "    gptscore[j]= gpt4o_eval_rs\n",
    "    multigptscore[j] = multiagent_eval_results\n",
    "    annomeanscore[j] = np.mean(annoscore[:,j,:],axis=0)\n",
    "\n",
    "    for human in range(num_human):\n",
    "        gptvsannos_spearman[human,j] = calculate_spearman_manual(gpt4o_eval_rs,annoscore[human,j,:])\n",
    "        multigptvsannos_spearman[human,j] = calculate_spearman_manual(multiagent_eval_results,annoscore[human,j,:])\n",
    "        baselinevsannos_spearman[human,j] = calculate_spearman_manual(baseline_rank[j],annoscore[human,j,:])\n",
    "\n",
    "    gptvsmultigpt_spearman[j] = calculate_spearman_manual(gpt4o_eval_rs,multiagent_eval_results)\n",
    "    gptvsannomean_spearman[j] = calculate_spearman_manual(gpt4o_eval_rs,annomeanscore[j])\n",
    "    multigptvsannomean_spearman[j] = calculate_spearman_manual(multiagent_eval_results,annomeanscore[j])\n",
    "    baselinevsannomean_spearman[j] = calculate_spearman_manual(baseline_rank[j],annomeanscore[j])\n",
    "\n",
    "    if gptvsannos_spearman[0,j] < 0.5:\n",
    "        badeval.append(i)\n",
    "\n",
    "avggptscore = gptscore.mean(axis=0)\n",
    "avgmultigptscore = multigptscore.mean(axis=0)\n",
    "\n",
    "\n",
    "if iternum%10==0:\n",
    "    print(\"GPT average score: \",avggptscore)\n",
    "    for i in range(num_human):\n",
    "        print(\"Anno{} average score: \".format(i+1),annoscore[i].mean(axis=0))\n",
    "\n",
    "\n",
    "# x = 0\n",
    "# y = 0\n",
    "# for i in range(len(models)):\n",
    "#     print(\"gpt eval gap in model {}\".format(models[i]),np.round(avggptscore[i] - annoscore[:num_human].mean(axis=0).mean(axis=0)[i],3))\n",
    "#     print(\"multi agent eval gap in model {}\".format(models[i]),np.round(avgmultigptscore[i] - annoscore[:num_human].mean(axis=0).mean(axis=0)[i],3))\n",
    "#     x += np.abs(np.round(avggptscore[i] - annoscore[:num_human].mean(axis=0).mean(axis=0)[i],3))\n",
    "#     y += np.abs(np.round(avgmultigptscore[i] - annoscore[:num_human].mean(axis=0).mean(axis=0)[i],3))\n",
    "# print(\"gpt eval gap in total\",x)\n",
    "# print(\"multi agent eval gap in total\",y)\n",
    "\n",
    "print(\"AnnoMean average score:\", np.round(annoscore[:num_human].mean(axis=0).mean(axis=0), 3))\n",
    "print(\"GPT average score:\", np.round(avggptscore, 3))\n",
    "print(\"MultiGPT average score:\", np.round(avgmultigptscore, 3))\n",
    "print(\"AnnoMean score rakn\",rank_numbers(annoscore[:num_human].mean(axis=0).mean(axis=0)))\n",
    "print(\"GPT score rank\",rank_numbers(avggptscore))\n",
    "print(\"MultiGPT score rank\",rank_numbers(avgmultigptscore))\n",
    "\n",
    "for i in range(num_human):\n",
    "    print(\"Anno{} average score:\".format(i + 1), np.round(annoscore[i].mean(axis=0), 3))\n",
    "\n",
    "print(\"GPT vs MultiGPT Spearman:\", np.round(gptvsmultigpt_spearman.mean(), 3))\n",
    "for i in range(num_human):\n",
    "    print(\"GPT vs Anno{} Spearman:\".format(i + 1), np.round(gptvsannos_spearman[i].mean(), 3))\n",
    "for i in range(num_human):\n",
    "    print(\"MultiGPT vs Anno{} Spearman:\".format(i + 1), np.round(multigptvsannos_spearman[i].mean(), 3))\n",
    "for i in range(num_human):\n",
    "    print(\"Baseline vs Anno{} Spearman:\".format(i + 1), np.round(baselinevsannos_spearman[i].mean(), 3))\n",
    "\n",
    "\n",
    "print(\"Average GPT vs Anno Spearman:\", np.round((gptvsannos_spearman[0].mean() + gptvsannos_spearman[1].mean() + gptvsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"GPT vs AnnoMean Spearman:\", np.round(gptvsannomean_spearman.mean(), 3))\n",
    "print(\"Average MultiGPT vs Anno Spearman:\", np.round((multigptvsannos_spearman[0].mean() + multigptvsannos_spearman[1].mean() + multigptvsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"MultiGPT vs AnnoMean Spearman:\", np.round(multigptvsannomean_spearman.mean(), 3))\n",
    "print(\"Average Baseline vs Anno Spearman:\", np.round((baselinevsannos_spearman[0].mean() + baselinevsannos_spearman[1].mean() + baselinevsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"Baseline vs AnnoMean Spearman:\", np.round(baselinevsannomean_spearman.mean(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.68662241 0.69532204 0.63927045]\n",
      " [0.68662241 1.         0.53052503 0.56169872]\n",
      " [0.69532204 0.53052503 1.         0.55360958]\n",
      " [0.63927045 0.56169872 0.55360958 1.        ]]\n",
      "0.6111747049247049\n"
     ]
    }
   ],
   "source": [
    "num_human_anno = 4\n",
    "spearmans =np.zeros([num_human_anno,num_human_anno,length])\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    for k in range(num_human_anno):\n",
    "        for l in range(num_human_anno):\n",
    "            spearmans[k,l,j] = calculate_spearman_manual(annoscore[k,j,:],annoscore[l,j,:])\n",
    "average_spearmans = spearmans.mean(axis=2)\n",
    "\n",
    "print(average_spearmans)\n",
    "print((average_spearmans.sum()-num_human_anno)/(num_human_anno*(num_human_anno-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    for index,model in enumerate(oc[i]['human_anno']):\n",
    "        oc[i]['human_anno'][model] = annoscore[:,j,index].astype(int).tolist()\n",
    "\n",
    "new_jsonpath = '../Human_anno/final/{}.json'.format(dimension)\n",
    "\n",
    "with open(new_jsonpath,'w') as f:\n",
    "    json.dump(oc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
