{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# anno_1 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "# anno_2 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "\n",
    "# anno_1oc =pd.read_excel(anno_1, usecols=['Model','scene'])\n",
    "# anno_2oc =pd.read_excel(anno_2, usecols=['Model','scene'])\n",
    "# anno_1oc = anno_1oc.astype(object).where(pd.notnull(anno_1oc), None)\n",
    "# anno_2oc = anno_2oc.astype(object).where(pd.notnull(anno_2oc), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr\n",
    "\n",
    "def rank_basescore(base_score, draw_ratio):\n",
    "    # 计算最大值和最小值的差值\n",
    "    score_range = max(base_score) - min(base_score)\n",
    "    # 计算平序的阈值\n",
    "    draw_gap = draw_ratio * score_range\n",
    "\n",
    "    # 对列表进行排序并保留原始索引\n",
    "    indexed_scores = list(enumerate(base_score))\n",
    "    indexed_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 处理平序\n",
    "    ranks = [0] * len(base_score)\n",
    "    current_rank = 1\n",
    "    for i in range(len(indexed_scores)):\n",
    "        if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "        else:\n",
    "            current_rank = i + 1\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'overall_consistency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gptvsanno1_spearman = 0\n",
    "# gptvsanno2_spearman = 0\n",
    "# anno1vsanno2_spearman = 0\n",
    "# gptvsannomean = 0\n",
    "# badeval = []\n",
    "\n",
    "# for i in range(length):\n",
    "#     if i % 3 == 0:\n",
    "#         gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 = human_anno[:,0]\n",
    "#         anno2 = human_anno[:,1]\n",
    "#         annomean = (anno1 + anno2)/2\n",
    "#     else:\n",
    "#         gpt4o_eval_rs+= np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 += human_anno[:,0]\n",
    "#         anno2 += human_anno[:,1]\n",
    "#         annomean += (anno1 + anno2)/2\n",
    "\n",
    "#     if i % 3 == 2:\n",
    "#         gptvsanno1_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "#         gptvsanno2_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "#         anno1vsanno2_spearman += calculate_spearman_manual(anno1,anno2)\n",
    "#         gptvsannomean += calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "\n",
    "# gptvsanno1_spearman = 3*gptvsanno1_spearman/length\n",
    "# gptvsanno2_spearman = 3*gptvsanno2_spearman/length\n",
    "# anno1vsanno2_spearman = 3*  anno1vsanno2_spearman/length\n",
    "# gptvsannomean = 3*gptvsannomean/length\n",
    "# print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman)\n",
    "# print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman)\n",
    "# print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman)\n",
    "# print(\"GPT vs AnnoMean Spearman: \",gptvsannomean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kling']\n",
    "idexls = []\n",
    "# for i in range(0,len(oc),3):\n",
    "#     idexls.append(i)\n",
    "for i in range(1,len(oc),3):\n",
    "    idexls.append(i)\n",
    "length = len(idexls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT vs Anno1 Spearman:  0.7324862637362637\n",
      "GPT vs Anno2 Spearman:  0.4252845368916798\n",
      "Anno1 vs Anno2 Spearman:  0.4672782574568288\n",
      "GPT vs AnnoMean Spearman:  0.6555631868131868\n",
      "Baseline vs Anno1 Spearman:  0.0\n",
      "Baseline vs Anno2 Spearman:  0.0\n",
      "Baseline vs AnnoMean Spearman:  0.0\n",
      "GPT average score:  [4.41758242 4.15934066 3.56043956 3.38461538 3.35164835 3.5\n",
      " 2.65934066]\n",
      "Anno1 average score:  [4.56043956 4.31868132 4.04395604 3.86813187 3.35164835 3.68681319\n",
      " 3.07142857]\n",
      "Anno2 average score:  [3.15934066 2.7967033  2.79120879 2.3956044  2.20879121 2.32967033\n",
      " 1.77472527]\n",
      "AnnoMean average score:  [3.85989011 3.55769231 3.41758242 3.13186813 2.78021978 3.00824176\n",
      " 2.42307692]\n"
     ]
    }
   ],
   "source": [
    "gptvsanno1_spearman = np.zeros(length)\n",
    "gptvsanno2_spearman = np.zeros(length)\n",
    "anno1vsanno2_spearman = np.zeros(length)\n",
    "gptvsannomean_spearman = np.zeros(length)\n",
    "baselinevsanno1_spearman = np.zeros(length)\n",
    "baselinevsanno2_spearman = np.zeros(length)\n",
    "baselinevsannomean_spearman = np.zeros(length)\n",
    "\n",
    "gptscore = np.zeros([len(models)])\n",
    "anno1score = np.zeros([len(models)])\n",
    "anno2score = np.zeros([len(models)])\n",
    "annomeanscore =np.zeros([len(models)])\n",
    "baseline_rank = np.zeros([7])\n",
    "badeval = []\n",
    "\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    # baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "\n",
    "\n",
    "    anno1 = human_anno[:,0]\n",
    "    # anno2 = human_anno[:,1]\n",
    "    anno2 = human_anno[:,3]\n",
    "\n",
    "    # gpt4o_eval_rs[gpt4o_eval_rs == 4] = 5\n",
    "    # anno1[anno1 == 4] = 5\n",
    "    # anno2[anno2 == 4] = 5\n",
    "    # gpt4o_eval_rs[gpt4o_eval_rs == 2] =1\n",
    "    # anno1[anno1 == 2] = 1\n",
    "    # anno2[anno2 == 2] = 1\n",
    "\n",
    "    annomean = (anno1 + anno2)/2\n",
    "\n",
    "\n",
    "    gptscore += gpt4o_eval_rs\n",
    "    anno1score += anno1\n",
    "    anno2score += anno2\n",
    "    annomeanscore += annomean\n",
    "    # baseline_rank += rank_basescore(baseline_score, 0.5)\n",
    "\n",
    "\n",
    "    gptvsanno1 = calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "    gptvsanno2 = calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "    anno1vsanno2 = calculate_spearman_manual(anno1,anno2)\n",
    "    gptvsannomean = calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "    # baselinevsanno1 = calculate_spearman_manual(baseline_rank,anno1)\n",
    "    # baselinevsanno2 = calculate_spearman_manual(baseline_rank,anno2)\n",
    "    # baselinevsannomean = calculate_spearman_manual(baseline_rank,annomean)\n",
    "\n",
    "    # j = i\n",
    "    gptvsanno1_spearman[j] = gptvsanno1\n",
    "    gptvsanno2_spearman[j] = gptvsanno2\n",
    "    anno1vsanno2_spearman[j] = anno1vsanno2\n",
    "    gptvsannomean_spearman[j] = gptvsannomean\n",
    "    # baselinevsanno1_spearman[j] = baselinevsanno1\n",
    "    # baselinevsanno2_spearman[j] = baselinevsanno2\n",
    "    # baselinevsannomean_spearman[j] = baselinevsannomean\n",
    "\n",
    "    if gptvsanno1 <0.2:\n",
    "        badeval.append(i)\n",
    "\n",
    "gptscore = gptscore/length\n",
    "anno1score = anno1score/length\n",
    "anno2score = anno2score/length\n",
    "annomeanscore = annomeanscore/length\n",
    "\n",
    "print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman.mean())\n",
    "print(\"GPT vs AnnoMean Spearman: \",gptvsannomean_spearman.mean())\n",
    "print(\"Baseline vs Anno1 Spearman: \",baselinevsanno1_spearman.mean())\n",
    "print(\"Baseline vs Anno2 Spearman: \",baselinevsanno2_spearman.mean())\n",
    "print(\"Baseline vs AnnoMean Spearman: \",baselinevsannomean_spearman.mean())\n",
    "\n",
    "\n",
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"Anno1 average score: \",anno1score)\n",
    "print(\"Anno2 average score: \",anno2score)\n",
    "print(\"AnnoMean average score: \",annomeanscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 69 Time lapse of sunrise on mars. 0.9196428571428571\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 231 is out of bounds for axis 0 with size 182",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m badeval:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# print(\"spearman\",gptvsannomean_spearman[i])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(\"videos\",oc[i]['videos']['gen2'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# print(\"prompt\",oc[i]['prompt_en'])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     badeval_prompt\u001b[38;5;241m.\u001b[39mappend(oc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_en\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m3\u001b[39m,i,oc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_en\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[43mgptvsanno2_spearman\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# print(\"GPT score\",oc[i]['gpt4o_eval'])\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# print(\"Anno score\",oc[i]['human_anno'])\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# print(\"GPT reasons\",gpt4o_eval_history[str(i)])\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 231 is out of bounds for axis 0 with size 182"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "badeval_prompt = []\n",
    "for i in badeval:\n",
    "    # print(\"spearman\",gptvsannomean_spearman[i])\n",
    "# print(\"videos\",oc[i]['videos']['gen2'])\n",
    "    # print(\"prompt\",oc[i]['prompt_en'])\n",
    "    badeval_prompt.append(oc[i]['prompt_en'])\n",
    "    print(i%3,i,oc[i]['prompt_en'],gptvsanno2_spearman[i])\n",
    "    # print(\"GPT score\",oc[i]['gpt4o_eval'])\n",
    "    # print(\"Anno score\",oc[i]['human_anno'])\n",
    "    # print(\"GPT reasons\",gpt4o_eval_history[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conuter = Counter(badeval_prompt)\n",
    "repeated = {k:v for k,v in conuter.items() if v>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(repeated.values()))\n",
    "# print(repeated)\n",
    "# with open('D:\\Astudying\\VideoEval\\HighlyHumanLikeVEB\\Human_anno\\BadEval4OverallConsistency.json','w') as f:\n",
    "#     json.dump(repeated,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1 7 A blue bicycle 0.0267857142857143\\n1 19 A pink bicycle 0.0803571428571429\\n1 22 A black bicycle 0.0982142857142857\\n2 44 A purple car 0.1964285714285714\\n1 49 A black car 0.1696428571428571\\n1 55 A red bird -0.0535714285714286\\n0 63 A yellow bird -0.0625\\n2 65 A yellow bird -0.1160714285714286\\n1 73 A pink bird 0.044642857142857095\\n2 92 A yellow cat -0.2857142857142858\\n1 94 A red umbrella 0.0357142857142857\\n2 107 An orange umbrella -0.03571428571428581\\n0 114 A black umbrella 0.0803571428571429\\n1 133 An orange suitcase 0.0803571428571429\\n2 134 An orange suitcase 0.1517857142857143\\n0 135 A purple suitcase 0.1785714285714286\\n0 147 A red bowl 0.1339285714285714\\n2 149 A red bowl 0.0535714285714286\\n2 179 A green chair -0.0625\\n2 191 A purple chair 0.0982142857142857\\n0 192 A pink chair 0.008928571428571397\\n1 199 A white chair 0.1964285714285714\\n0 204 A green clock 0.1785714285714286\\n1 205 A green clock 0.1071428571428571\\n0 210 A yellow clock 0.0892857142857143\\n2 218 A purple clock -0.0535714285714286\\n2 233 A green vase 0.044642857142857095\\n1 247 A pink vase -0.09821428571428581\\n1 250 A black vase 0.0535714285714286'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1 7 A blue bicycle 0.0267857142857143\n",
    "1 19 A pink bicycle 0.0803571428571429\n",
    "1 22 A black bicycle 0.0982142857142857\n",
    "2 44 A purple car 0.1964285714285714\n",
    "1 49 A black car 0.1696428571428571\n",
    "1 55 A red bird -0.0535714285714286\n",
    "0 63 A yellow bird -0.0625\n",
    "2 65 A yellow bird -0.1160714285714286\n",
    "1 73 A pink bird 0.044642857142857095\n",
    "2 92 A yellow cat -0.2857142857142858\n",
    "1 94 A red umbrella 0.0357142857142857\n",
    "2 107 An orange umbrella -0.03571428571428581\n",
    "0 114 A black umbrella 0.0803571428571429\n",
    "1 133 An orange suitcase 0.0803571428571429\n",
    "2 134 An orange suitcase 0.1517857142857143\n",
    "0 135 A purple suitcase 0.1785714285714286\n",
    "0 147 A red bowl 0.1339285714285714\n",
    "2 149 A red bowl 0.0535714285714286\n",
    "2 179 A green chair -0.0625\n",
    "2 191 A purple chair 0.0982142857142857\n",
    "0 192 A pink chair 0.008928571428571397\n",
    "1 199 A white chair 0.1964285714285714\n",
    "0 204 A green clock 0.1785714285714286\n",
    "1 205 A green clock 0.1071428571428571\n",
    "0 210 A yellow clock 0.0892857142857143\n",
    "2 218 A purple clock -0.0535714285714286\n",
    "2 233 A green vase 0.044642857142857095\n",
    "1 247 A pink vase -0.09821428571428581\n",
    "1 250 A black vase 0.0535714285714286\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 18 A pink bicycle 0.4732142857142857\\n1 22 A black bicycle 0.0982142857142857\\n2 29 A red car 0.2232142857142857\\n0 30 A green car 0.7142857142857143\\n0 39 An orange car 0.7946428571428572\\n2 44 A purple car 0.1964285714285714\\n2 56 A red bird 0.375\\n0 57 A green bird 0.2857142857142857\\n0 63 A yellow bird -0.0625\\n1 73 A pink bird 0.044642857142857095\\n0 78 A white bird 0.5982142857142857\\n0 81 A black cat 0.4107142857142857\\n1 88 An orange cat 0.2410714285714286\\n2 89 An orange cat 0.4107142857142857\\n2 92 A yellow cat -0.2857142857142858\\n0 96 A green umbrella 0.3303571428571429\\n1 121 A red suitcase 0.7857142857142857\\n0 126 A blue suitcase 0.2232142857142857\\n1 151 A green bowl 0.6517857142857143\\n2 179 A green chair -0.0625\\n0 180 A blue chair 0.5535714285714286\\n1 181 A blue chair 0.3571428571428571\\n0 198 A white chair 0.5446428571428572\\n2 212 A yellow clock 0.2767857142857143\\n1 220 A pink clock 0.6785714285714286\\n0 225 A white clock 0.2589285714285714\\n1 226 A white clock 0.375\\n2 236 A blue vase 0.4285714285714286\\n1 250 A black vase 0.0535714285714286'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"0 18 A pink bicycle 0.4732142857142857\n",
    "1 22 A black bicycle 0.0982142857142857\n",
    "2 29 A red car 0.2232142857142857\n",
    "0 30 A green car 0.7142857142857143\n",
    "0 39 An orange car 0.7946428571428572\n",
    "2 44 A purple car 0.1964285714285714\n",
    "2 56 A red bird 0.375\n",
    "0 57 A green bird 0.2857142857142857\n",
    "0 63 A yellow bird -0.0625\n",
    "1 73 A pink bird 0.044642857142857095\n",
    "0 78 A white bird 0.5982142857142857\n",
    "0 81 A black cat 0.4107142857142857\n",
    "1 88 An orange cat 0.2410714285714286\n",
    "2 89 An orange cat 0.4107142857142857\n",
    "2 92 A yellow cat -0.2857142857142858\n",
    "0 96 A green umbrella 0.3303571428571429\n",
    "1 121 A red suitcase 0.7857142857142857\n",
    "0 126 A blue suitcase 0.2232142857142857\n",
    "1 151 A green bowl 0.6517857142857143\n",
    "2 179 A green chair -0.0625\n",
    "0 180 A blue chair 0.5535714285714286\n",
    "1 181 A blue chair 0.3571428571428571\n",
    "0 198 A white chair 0.5446428571428572\n",
    "2 212 A yellow clock 0.2767857142857143\n",
    "1 220 A pink clock 0.6785714285714286\n",
    "0 225 A white clock 0.2589285714285714\n",
    "1 226 A white clock 0.375\n",
    "2 236 A blue vase 0.4285714285714286\n",
    "1 250 A black vase 0.0535714285714286\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
