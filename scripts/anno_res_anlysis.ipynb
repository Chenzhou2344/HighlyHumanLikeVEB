{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr\n",
    "\n",
    "def rank_basescore(base_score, draw_ratio):\n",
    "    # 计算最大值和最小值的差值\n",
    "    score_range = max(base_score) - min(base_score)\n",
    "    # 计算平序的阈值\n",
    "    draw_gap = draw_ratio * score_range\n",
    "\n",
    "    # 对列表进行排序并保留原始索引\n",
    "    indexed_scores = list(enumerate(base_score))\n",
    "    indexed_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 处理平序\n",
    "    ranks = [0] * len(base_score)\n",
    "    current_rank = 1\n",
    "    for i in range(len(indexed_scores)):\n",
    "        if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "        else:\n",
    "            current_rank = i + 1\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'imaging_quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kl ng']\n",
    "# l1 = list(range(0,len(oc),3))\n",
    "# l2 = list(range(1,len(oc),3))\n",
    "# l3 = list(range(2,len(oc),3))\n",
    "l = list(range(0,len(oc),3))\n",
    "idexls = l\n",
    "\n",
    "length = len(idexls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iter max gap in gpt\n",
      "gpt score gap 0 0.9780219780219781 6\n",
      "multi agent socre gap0 0.9780219780219781 6\n",
      "gpt score gap 1 2.989010989010989 3\n",
      "multi agent socre gap1 2.989010989010989 3\n",
      "gpt score gap 2 1.1318681318681323 5\n",
      "multi agent socre gap2 1.1318681318681323 5\n",
      "gpt score gap 3 4.7032967032967035 1\n",
      "multi agent socre gap3 4.7032967032967035 1\n",
      "GPT vs MultiGPT Spearman: 1.0\n",
      "gpt eval gap in model cogvideox5b 0.341\n",
      "multi agent eval gap in model cogvideox5b 0.341\n",
      "gpt eval gap in model gen3 0.007\n",
      "multi agent eval gap in model gen3 0.007\n",
      "gpt eval gap in model kling 0.176\n",
      "multi agent eval gap in model kling 0.176\n",
      "gpt eval gap in model videocrafter2 1.04\n",
      "multi agent eval gap in model videocrafter2 1.04\n",
      "gpt eval gap in model pika 0.333\n",
      "multi agent eval gap in model pika 0.333\n",
      "gpt eval gap in model show1 0.769\n",
      "multi agent eval gap in model show1 0.769\n",
      "gpt eval gap in model lavie 1.176\n",
      "multi agent eval gap in model lavie 1.176\n",
      "gpt eval gap in total 3.8419999999999996\n",
      "multi agent eval gap in total 3.8419999999999996\n",
      "AnnoMean average score: [3.692 4.696 4.066 3.19  3.502 3.022 2.297]\n",
      "AnnoMean score rakn [3.0, 1.0, 2.0, 5.0, 4.0, 6.0, 7.0]\n",
      "GPT average score: [4.033 4.703 4.242 4.231 3.835 3.791 3.473]\n",
      "GPT score rank [4.0, 1.0, 2.0, 3.0, 5.0, 6.0, 7.0]\n",
      "MultiGPT average score: [4.033 4.703 4.242 4.231 3.835 3.791 3.473]\n",
      "MultiGPT score rank [4.0, 1.0, 2.0, 3.0, 5.0, 6.0, 7.0]\n",
      "Anno1 average score: [3.791 4.967 3.934 3.352 3.615 3.055 2.495]\n",
      "Anno2 average score: [2.286 4.121 3.264 1.242 1.967 1.088 1.044]\n",
      "Anno3 average score: [5.    5.    5.    4.978 4.923 4.923 3.352]\n",
      "GPT vs Anno1 Spearman: 0.649\n",
      "GPT vs Anno2 Spearman: 0.602\n",
      "GPT vs Anno3 Spearman: 0.661\n",
      "MultiGPT vs Anno1 Spearman: 0.649\n",
      "MultiGPT vs Anno2 Spearman: 0.602\n",
      "MultiGPT vs Anno3 Spearman: 0.661\n",
      "Baseline vs Anno1 Spearman: -0.57\n",
      "Baseline vs Anno2 Spearman: -0.591\n",
      "Baseline vs Anno3 Spearman: -0.081\n",
      "Average GPT vs Anno Spearman: 0.637\n",
      "GPT vs AnnoMean Spearman: 0.622\n",
      "Average MultiGPT vs Anno Spearman: 0.637\n",
      "MultiGPT vs AnnoMean Spearman: 0.622\n",
      "Average Baseline vs Anno Spearman: -0.414\n",
      "Baseline vs AnnoMean Spearman: -0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_4904\\1001780079.py:33: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  score_range = max(base_score) - min(base_score)\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_4904\\1001780079.py:39: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  indexed_scores.sort(key=lambda x: x[1])\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_4904\\1001780079.py:45: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "iternum = 0\n",
    "num_human = 3\n",
    "while(flag == 0):\n",
    "    iternum += 1\n",
    "    gptvsannos_spearman = np.zeros((4,length))\n",
    "    gptvsannomean_spearman = np.zeros(length)\n",
    "    multigptvsannos_spearman = np.zeros((4,length))\n",
    "    multigptvsannomean_spearman = np.zeros(length)\n",
    "    baselinevsannos_spearman = np.zeros((4,length))\n",
    "    baselinevsannomean_spearman = np.zeros(length)\n",
    "    gptvsmultigpt_spearman = np.zeros(length)\n",
    "\n",
    "\n",
    "    gptscore = np.zeros((length,len(models)))\n",
    "    annoscore = np.zeros((4,length,len(models)))\n",
    "    multigptscore = np.zeros((length,len(models)))\n",
    "    annomeanscore =np.zeros((length,len(models)))\n",
    "    baseline_rank = np.zeros((length,len(models)))\n",
    "    badeval = []\n",
    "\n",
    "    for j in range(length):\n",
    "        i = idexls[j]\n",
    "        gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "        human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "        baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "        # multiagent_eval_results = np.array(list(oc[i]['multiagent_score'].values()))\n",
    "        # multiagent_eval_results = np.array(list(oc[i]['combench_style'].values()))\n",
    "        multiagent_eval_results = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "\n",
    "        for human in range(num_human):\n",
    "            annoscore[human,j,:] = human_anno[:,human]\n",
    "\n",
    "        baseline_rank[j] = rank_basescore(baseline_score, 0.165)\n",
    "        gptscore[j]= gpt4o_eval_rs\n",
    "        multigptscore[j] = multiagent_eval_results\n",
    "        annomeanscore[j] = np.mean(annoscore[:,j,:],axis=0)\n",
    "\n",
    "        for human in range(num_human):\n",
    "            gptvsannos_spearman[human,j] = calculate_spearman_manual(gpt4o_eval_rs,annoscore[human,j,:])\n",
    "            multigptvsannos_spearman[human,j] = calculate_spearman_manual(multiagent_eval_results,annoscore[human,j,:])\n",
    "            baselinevsannos_spearman[human,j] = calculate_spearman_manual(baseline_rank[j],annoscore[human,j,:])\n",
    "\n",
    "        gptvsmultigpt_spearman[j] = calculate_spearman_manual(gpt4o_eval_rs,multiagent_eval_results)\n",
    "        gptvsannomean_spearman[j] = calculate_spearman_manual(gpt4o_eval_rs,annomeanscore[j])\n",
    "        multigptvsannomean_spearman[j] = calculate_spearman_manual(multiagent_eval_results,annomeanscore[j])\n",
    "        baselinevsannomean_spearman[j] = calculate_spearman_manual(baseline_rank[j],annomeanscore[j])\n",
    "\n",
    "    gptscore = gptscore.mean(axis=0)\n",
    "    multigptscore = multigptscore.mean(axis=0)\n",
    "    annomeanscore = annomeanscore.mean(axis=0)\n",
    "\n",
    "    print(\"{} iter max gap in gpt\".format(iternum))\n",
    "    for i in range(4):\n",
    "        print('gpt score gap {}'.format(i),np.max(np.abs(gptscore - annoscore[i].mean(axis=0))),np.argmax(np.abs(gptscore - annoscore[i].mean(axis=0))))\n",
    "        print('multi agent socre gap{}'.format(i),np.max(np.abs(multigptscore - annoscore[i].mean(axis=0))),np.argmax(np.abs(multigptscore - annoscore[i].mean(axis=0))))\n",
    "\n",
    "    # if np.max(np.abs(gptscore - anno1score) ) < 0.1 or np.max(np.abs(gptscore - anno2score)) < 0.1 or np.max(np.abs(gptscore - anno3score)) < 0.1:\n",
    "    flag = 1\n",
    "\n",
    "    if iternum%10==0:\n",
    "        print(\"GPT average score: \",gptscore)\n",
    "        for i in range(num_human):\n",
    "            print(\"Anno{} average score: \".format(i+1),annoscore[i].mean(axis=0))\n",
    "\n",
    "print(\"GPT vs MultiGPT Spearman:\", np.round(gptvsmultigpt_spearman.mean(), 3))\n",
    "\n",
    "x = 0\n",
    "y = 0\n",
    "for i in range(len(models)):\n",
    "    print(\"gpt eval gap in model {}\".format(models[i]),np.round(gptscore[i] - annoscore[:3].mean(axis=0).mean(axis=0)[i],3))\n",
    "    print(\"multi agent eval gap in model {}\".format(models[i]),np.round(multigptscore[i] - annoscore[:3].mean(axis=0).mean(axis=0)[i],3))\n",
    "    x += np.abs(np.round(gptscore[i] - annoscore[:3].mean(axis=0).mean(axis=0)[i],3))\n",
    "    y += np.abs(np.round(multigptscore[i] - annoscore[:3].mean(axis=0).mean(axis=0)[i],3))\n",
    "print(\"gpt eval gap in total\",x)\n",
    "print(\"multi agent eval gap in total\",y)\n",
    "\n",
    "print(\"AnnoMean average score:\", np.round(annoscore[:3].mean(axis=0).mean(axis=0), 3))\n",
    "print(\"AnnoMean score rakn\",rank_numbers(annoscore[:3].mean(axis=0).mean(axis=0)))\n",
    "print(\"GPT average score:\", np.round(gptscore, 3))\n",
    "print(\"GPT score rank\",rank_numbers(gptscore))\n",
    "print(\"MultiGPT average score:\", np.round(multigptscore, 3))\n",
    "print(\"MultiGPT score rank\",rank_numbers(multigptscore))\n",
    "\n",
    "for i in range(num_human):\n",
    "    print(\"Anno{} average score:\".format(i + 1), np.round(annoscore[i].mean(axis=0), 3))\n",
    "\n",
    "for i in range(num_human):\n",
    "    print(\"GPT vs Anno{} Spearman:\".format(i + 1), np.round(gptvsannos_spearman[i].mean(), 3))\n",
    "for i in range(num_human):\n",
    "    print(\"MultiGPT vs Anno{} Spearman:\".format(i + 1), np.round(multigptvsannos_spearman[i].mean(), 3))\n",
    "for i in range(num_human):\n",
    "    print(\"Baseline vs Anno{} Spearman:\".format(i + 1), np.round(baselinevsannos_spearman[i].mean(), 3))\n",
    "\n",
    "\n",
    "print(\"Average GPT vs Anno Spearman:\", np.round((gptvsannos_spearman[0].mean() + gptvsannos_spearman[1].mean() + gptvsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"GPT vs AnnoMean Spearman:\", np.round(gptvsannomean_spearman.mean(), 3))\n",
    "print(\"Average MultiGPT vs Anno Spearman:\", np.round((multigptvsannos_spearman[0].mean() + multigptvsannos_spearman[1].mean() + multigptvsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"MultiGPT vs AnnoMean Spearman:\", np.round(multigptvsannomean_spearman.mean(), 3))\n",
    "print(\"Average Baseline vs Anno Spearman:\", np.round((baselinevsannos_spearman[0].mean() + baselinevsannos_spearman[1].mean() + baselinevsannos_spearman[2].mean()) / 3, 3))\n",
    "print(\"Baseline vs AnnoMean Spearman:\", np.round(baselinevsannomean_spearman.mean(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.79042386 0.67278257 0.57231162]\n",
      " [0.79042386 1.         0.61813187 0.56897567]\n",
      " [0.67278257 0.61813187 1.         0.8021978 ]\n",
      " [0.57231162 0.56897567 0.8021978  1.        ]]\n",
      "0.6708038984824699\n"
     ]
    }
   ],
   "source": [
    "num_human_anno = 4\n",
    "spearmans =np.zeros([num_human_anno,num_human_anno,length])\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    for k in range(num_human_anno):\n",
    "        for l in range(num_human_anno):\n",
    "            spearmans[k,l,j] = calculate_spearman_manual(annoscore[k,j,:],annoscore[l,j,:])\n",
    "average_spearmans = spearmans.mean(axis=2)\n",
    "\n",
    "print(average_spearmans)\n",
    "print((average_spearmans.sum()-num_human_anno)/(num_human_anno*(num_human_anno-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
