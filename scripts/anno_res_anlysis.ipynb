{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# anno_1 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "# anno_2 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "\n",
    "# anno_1oc =pd.read_excel(anno_1, usecols=['Model','scene'])\n",
    "# anno_2oc =pd.read_excel(anno_2, usecols=['Model','scene'])\n",
    "# anno_1oc = anno_1oc.astype(object).where(pd.notnull(anno_1oc), None)\n",
    "# anno_2oc = anno_2oc.astype(object).where(pd.notnull(anno_2oc), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'overall_consistency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gptvsanno1_spearman = 0\n",
    "# gptvsanno2_spearman = 0\n",
    "# anno1vsanno2_spearman = 0\n",
    "# gptvsannomean = 0\n",
    "# badeval = []\n",
    "\n",
    "# for i in range(length):\n",
    "#     if i % 3 == 0:\n",
    "#         gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 = human_anno[:,0]\n",
    "#         anno2 = human_anno[:,1]\n",
    "#         annomean = (anno1 + anno2)/2\n",
    "#     else:\n",
    "#         gpt4o_eval_rs+= np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 += human_anno[:,0]\n",
    "#         anno2 += human_anno[:,1]\n",
    "#         annomean += (anno1 + anno2)/2\n",
    "\n",
    "#     if i % 3 == 2:\n",
    "#         gptvsanno1_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "#         gptvsanno2_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "#         anno1vsanno2_spearman += calculate_spearman_manual(anno1,anno2)\n",
    "#         gptvsannomean += calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "\n",
    "# gptvsanno1_spearman = 3*gptvsanno1_spearman/length\n",
    "# gptvsanno2_spearman = 3*gptvsanno2_spearman/length\n",
    "# anno1vsanno2_spearman = 3*  anno1vsanno2_spearman/length\n",
    "# gptvsannomean = 3*gptvsannomean/length\n",
    "# print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman)\n",
    "# print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman)\n",
    "# print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman)\n",
    "# print(\"GPT vs AnnoMean Spearman: \",gptvsannomean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kling']\n",
    "idexls = []\n",
    "for i in range(0,len(oc),3):\n",
    "    idexls.append(i)\n",
    "length = len(idexls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT vs Anno1 Spearman:  0.7558869701726846\n",
      "GPT vs Anno2 Spearman:  0.390894819466248\n",
      "Anno1 vs Anno2 Spearman:  0.4745879120879122\n",
      "GPT vs AnnoMean Spearman:  0.6387362637362637\n",
      "GPT average score:  [4.41758242 4.18681319 3.47252747 3.65934066 3.82417582 3.84615385\n",
      " 2.92307692]\n",
      "Anno1 average score:  [4.56043956 4.30769231 3.93406593 3.8021978  3.43956044 3.67032967\n",
      " 3.10989011]\n",
      "Anno2 average score:  [2.93406593 2.59340659 2.53846154 2.10989011 1.93406593 2.04395604\n",
      " 1.61538462]\n",
      "AnnoMean average score:  [3.74725275 3.45054945 3.23626374 2.95604396 2.68681319 2.85714286\n",
      " 2.36263736]\n"
     ]
    }
   ],
   "source": [
    "gptvsanno1_spearman = np.zeros(length)\n",
    "gptvsanno2_spearman = np.zeros(length)\n",
    "anno1vsanno2_spearman = np.zeros(length)\n",
    "gptvsannomean_spearman = np.zeros(length)\n",
    "baselinevsanno1_spearman = np.zeros(length)\n",
    "baselinevsanno2_spearman = np.zeros(length)\n",
    "baselinevsannomean_spearman = np.zeros(length)\n",
    "\n",
    "gptscore = np.zeros([len(models)])\n",
    "anno1score = np.zeros([len(models)])\n",
    "anno2score = np.zeros([len(models)])\n",
    "annomeanscore =np.zeros([len(models)])\n",
    "# baseline_rank = np.zeros([7])\n",
    "badeval = []\n",
    "\n",
    "for i in idexls:\n",
    "    gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    # baseline_rank = np.array(list(oc[i]['baseline_rank'].values())[3:])\n",
    "\n",
    "    anno1 = human_anno[:,0]\n",
    "    # anno2 = human_anno[:,1]\n",
    "    anno2 = human_anno[:,3]\n",
    "\n",
    "    # gpt4o_eval_rs[gpt4o_eval_rs == 4] = 5\n",
    "    # anno1[anno1 == 4] = 5\n",
    "    # anno2[anno2 == 4] = 5\n",
    "\n",
    "    annomean = (anno1 + anno2)/2\n",
    "\n",
    "\n",
    "    gptscore += gpt4o_eval_rs\n",
    "    anno1score += anno1\n",
    "    anno2score += anno2\n",
    "    annomeanscore += annomean\n",
    "\n",
    "\n",
    "    gptvsanno1 = calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "    gptvsanno2 = calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "    anno1vsanno2 = calculate_spearman_manual(anno1,anno2)\n",
    "    gptvsannomean = calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "    # baselinevsanno1 = calculate_spearman_manual(baseline_rank,anno1)\n",
    "    # baselinevsanno2 = calculate_spearman_manual(baseline_rank,anno2)\n",
    "    # baselinevsannomean = calculate_spearman_manual(baseline_rank,annomean)\n",
    "\n",
    "    j = int(i/3)\n",
    "    # j = i\n",
    "    gptvsanno1_spearman[j] = gptvsanno1\n",
    "    gptvsanno2_spearman[j] = gptvsanno2\n",
    "    anno1vsanno2_spearman[j] = anno1vsanno2\n",
    "    gptvsannomean_spearman[j] = gptvsannomean\n",
    "    # baselinevsanno1_spearman[i] = baselinevsanno1\n",
    "    # baselinevsanno2_spearman[i] = baselinevsanno2\n",
    "    # baselinevsannomean_spearman[i] = baselinevsannomean\n",
    "\n",
    "    if gptvsanno2 <0.2:\n",
    "        badeval.append(i)\n",
    "\n",
    "gptscore = gptscore/length\n",
    "anno1score = anno1score/length\n",
    "anno2score = anno2score/length\n",
    "annomeanscore = annomeanscore/length\n",
    "\n",
    "print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman.mean())\n",
    "print(\"GPT vs AnnoMean Spearman: \",gptvsannomean_spearman.mean())\n",
    "# print(\"Baseline vs Anno1 Spearman: \",baselinevsanno1_spearman.mean())\n",
    "# print(\"Baseline vs Anno2 Spearman: \",baselinevsanno2_spearman.mean())\n",
    "# print(\"Baseline vs AnnoMean Spearman: \",baselinevsannomean_spearman.mean())\n",
    "\n",
    "\n",
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"Anno1 average score: \",anno1score)\n",
    "print(\"Anno2 average score: \",anno2score)\n",
    "print(\"AnnoMean average score: \",annomeanscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Turtle swimming in ocean. 0.0625\n",
      "0 9 A panda standing on a surfboard in the ocean in sunset. 0.625\n",
      "0 12 An astronaut feeding ducks on a sunny afternoon, reflection from the water. 0.2053571428571429\n",
      "0 15 Two pandas discussing an academic paper. 0.9196428571428571\n",
      "0 18 Sunset time lapse at the beach with moving clouds and colors in the sky. -0.1696428571428572\n",
      "0 30 Fireworks. 0.6517857142857143\n",
      "0 33 An animated painting of fluffy white clouds moving in sky. 0.7767857142857143\n",
      "0 54 An ice cream is melting on the table. 0.4910714285714286\n",
      "0 66 A teddy bear is swimming in the ocean. 0.6517857142857143\n",
      "0 87 Campfire at night in a snowy forest with starry sky in the background. 0.8303571428571428\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 93 is out of bounds for axis 0 with size 91",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m badeval:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# print(\"spearman\",gptvsannomean_spearman[i])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(\"videos\",oc[i]['videos']['gen2'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# print(\"prompt\",oc[i]['prompt_en'])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     badeval_prompt\u001b[38;5;241m.\u001b[39mappend(oc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_en\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m3\u001b[39m,i,oc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_en\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[43mgptvsanno2_spearman\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 93 is out of bounds for axis 0 with size 91"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "badeval_prompt = []\n",
    "for i in badeval:\n",
    "    # print(\"spearman\",gptvsannomean_spearman[i])\n",
    "# print(\"videos\",oc[i]['videos']['gen2'])\n",
    "    # print(\"prompt\",oc[i]['prompt_en'])\n",
    "    badeval_prompt.append(oc[i]['prompt_en'])\n",
    "    print(i%3,i,oc[i]['prompt_en'],gptvsanno2_spearman[i])\n",
    "    # print(\"GPT score\",oc[i]['gpt4o_eval'])\n",
    "    # print(\"Anno score\",oc[i]['human_anno'])\n",
    "    # print(\"GPT reasons\",gpt4o_eval_history[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conuter = Counter(badeval_prompt)\n",
    "repeated = {k:v for k,v in conuter.items() if v>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(repeated.values()))\n",
    "# print(repeated)\n",
    "# with open('D:\\Astudying\\VideoEval\\HighlyHumanLikeVEB\\Human_anno\\BadEval4OverallConsistency.json','w') as f:\n",
    "#     json.dump(repeated,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
