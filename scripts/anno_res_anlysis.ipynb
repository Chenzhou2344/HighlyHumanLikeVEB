{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr\n",
    "\n",
    "def rank_basescore(base_score, draw_ratio):\n",
    "    # 计算最大值和最小值的差值\n",
    "    score_range = max(base_score) - min(base_score)\n",
    "    # 计算平序的阈值\n",
    "    draw_gap = draw_ratio * score_range\n",
    "\n",
    "    # 对列表进行排序并保留原始索引\n",
    "    indexed_scores = list(enumerate(base_score))\n",
    "    indexed_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 处理平序\n",
    "    ranks = [0] * len(base_score)\n",
    "    current_rank = 1\n",
    "    for i in range(len(indexed_scores)):\n",
    "        if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "        else:\n",
    "            current_rank = i + 1\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kling']\n",
    "idexls = []\n",
    "for i in range(0,len(oc)):\n",
    "    idexls.append(i)\n",
    "# for i in range(1,len(oc),3):\n",
    "#     idexls.append(i)\n",
    "length = len(idexls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iter max gap in gpt\n",
      "0.1098039215686275 0\n",
      "0.19607843137254877 4\n",
      "0.17647058823529393 4\n",
      "0.34117647058823497 4\n",
      "0.11764705882352944 0\n",
      "0.2274509803921565 4\n",
      "2.8823529411764706 3\n",
      "2.937254901960784 0\n",
      "GPT average score:  [2.83921569 2.83529412 2.87843137 2.88235294 2.37647059 2.80784314\n",
      " 2.74509804]\n",
      "MultiGPT average score:  [2.9372549  2.83137255 2.90980392 2.89803922 2.54117647 2.83137255\n",
      " 2.8627451 ]\n",
      "Anno1 average score:  [2.94901961 2.87843137 2.84313725 2.88627451 2.34509804 2.75686275\n",
      " 2.75686275]\n",
      "Anno2 average score:  [2.87843137 2.90196078 2.8745098  2.84705882 2.2        2.73333333\n",
      " 2.57647059]\n",
      "Anno3 average score:  [2.95686275 2.9372549  2.90196078 2.90980392 2.31372549 2.87058824\n",
      " 2.79215686]\n",
      "AnnoMean average score:  [2.19607843 2.17941176 2.15490196 2.16078431 1.71470588 2.09019608\n",
      " 2.03137255]\n",
      "GPT vs Anno1 Spearman:  0.7965336134453782\n",
      "GPT vs Anno2 Spearman:  0.7609243697478991\n",
      "GPT vs Anno3 Spearman:  0.8192577030812325\n",
      "MultiGPT vs Anno1 Spearman:  0.835889355742297\n",
      "MultiGPT vs Anno2 Spearman:  0.7593487394957984\n",
      "MultiGPT vs Anno3 Spearman:  0.8469887955182074\n",
      "Baseline vs Anno1 Spearman:  0.5787464985994398\n",
      "Baseline vs Anno2 Spearman:  0.5076680672268908\n",
      "Baseline vs Anno3 Spearman:  0.592296918767507\n",
      "Average GPT vs Anno Spearman:  0.7922385620915033\n",
      "GPT vs AnnoMean Spearman:  0.745203081232493\n",
      "Average MultiGPT vs Anno Spearman:  0.814075630252101\n",
      "MultiGPT vs AnnoMean Spearman:  0.7499649859943979\n",
      "Average Baseline vs Anno Spearman:  0.5595704948646126\n",
      "Baseline vs AnnoMean Spearman:  0.4676120448179272\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "iternum = 0\n",
    "num_human = 3\n",
    "while(flag == 0):\n",
    "    iternum += 1\n",
    "    gptvsannos_spearman = np.zeros((4,length))\n",
    "    gptvsannomean_spearman = np.zeros(length)\n",
    "    multigptvsannos_spearman = np.zeros((4,length))\n",
    "    multigptvsannomean_spearman = np.zeros(length)\n",
    "    baselinevsannos_spearman = np.zeros((4,length))\n",
    "    baselinevsannomean_spearman = np.zeros(length)\n",
    "\n",
    "    gptscore = np.zeros((length,len(models)))\n",
    "    annoscore = np.zeros((4,length,len(models)))\n",
    "    multigptscore = np.zeros((length,len(models)))\n",
    "    annomeanscore =np.zeros((length,len(models)))\n",
    "    baseline_rank = np.zeros((length,len(models)))\n",
    "    badeval = []\n",
    "\n",
    "    for j in range(length):\n",
    "        i = idexls[j]\n",
    "        gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "        human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "        baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "        multiagent_eval_results = np.array(list(oc[i]['multiagent_score'].values()))\n",
    "        # multiagent_eval_results = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "\n",
    "        for human in range(num_human):\n",
    "            annoscore[human,j,:] = human_anno[:,human]\n",
    "\n",
    "        baseline_rank[j] = rank_basescore(baseline_score, 0.165)\n",
    "        gptscore[j]= gpt4o_eval_rs\n",
    "        multigptscore[j] = multiagent_eval_results\n",
    "        annomeanscore[j] = np.mean(annoscore[:,j,:],axis=0)\n",
    "\n",
    "        for human in range(num_human):\n",
    "            gptvsannos_spearman[human,j] = calculate_spearman_manual(gpt4o_eval_rs,annoscore[human,j,:])\n",
    "            multigptvsannos_spearman[human,j] = calculate_spearman_manual(multiagent_eval_results,annoscore[human,j,:])\n",
    "            baselinevsannos_spearman[human,j] = calculate_spearman_manual(baseline_rank[j],annoscore[human,j,:])\n",
    "\n",
    "        gptvsannomean_spearman[j] = calculate_spearman_manual(gpt4o_eval_rs,annomeanscore[j])\n",
    "        multigptvsannomean_spearman[j] = calculate_spearman_manual(multiagent_eval_results,annomeanscore[j])\n",
    "        baselinevsannomean_spearman[j] = calculate_spearman_manual(baseline_rank[j],annomeanscore[j])\n",
    "\n",
    "    gptscore = gptscore.mean(axis=0)\n",
    "    multigptscore = multigptscore.mean(axis=0)\n",
    "    annomeanscore = annomeanscore.mean(axis=0)\n",
    "\n",
    "    print(\"{} iter max gap in gpt\".format(iternum))\n",
    "    for i in range(4):\n",
    "        print(np.max(np.abs(gptscore - annoscore[i].mean(axis=0))),np.argmax(np.abs(gptscore - annoscore[i].mean(axis=0))))\n",
    "        print(np.max(np.abs(multigptscore - annoscore[i].mean(axis=0))),np.argmax(np.abs(multigptscore - annoscore[i].mean(axis=0))))\n",
    "\n",
    "    # if np.max(np.abs(gptscore - anno1score) ) < 0.1 or np.max(np.abs(gptscore - anno2score)) < 0.1 or np.max(np.abs(gptscore - anno3score)) < 0.1:\n",
    "    flag = 1\n",
    "\n",
    "    if iternum%10==0:\n",
    "        print(\"GPT average score: \",gptscore)\n",
    "        for i in range(num_human):\n",
    "            print(\"Anno{} average score: \".format(i+1),annoscore[i].mean(axis=0))\n",
    "\n",
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"MultiGPT average score: \",multigptscore)\n",
    "for i in range(num_human):\n",
    "    print(\"Anno{} average score: \".format(i+1),annoscore[i].mean(axis=0))\n",
    "print(\"AnnoMean average score: \",annomeanscore)\n",
    "\n",
    "for i in range(num_human):\n",
    "    print(\"GPT vs Anno{} Spearman: \".format(i+1),gptvsannos_spearman[i].mean())\n",
    "for i in range(num_human):\n",
    "    print(\"MultiGPT vs Anno{} Spearman: \".format(i+1),multigptvsannos_spearman[i].mean())\n",
    "for i in range(num_human):\n",
    "    print(\"Baseline vs Anno{} Spearman: \".format(i+1),baselinevsannos_spearman[i].mean())\n",
    "    \n",
    "print(\"Average GPT vs Anno Spearman: \",(gptvsannos_spearman[0].mean()+gptvsannos_spearman[1].mean()+gptvsannos_spearman[2].mean())/3)\n",
    "print(\"GPT vs AnnoMean Spearman: \",gptvsannomean_spearman.mean())\n",
    "print(\"Average MultiGPT vs Anno Spearman: \",(multigptvsannos_spearman[0].mean()+multigptvsannos_spearman[1].mean()+multigptvsannos_spearman[2].mean())/3)\n",
    "print(\"MultiGPT vs AnnoMean Spearman: \",multigptvsannomean_spearman.mean())\n",
    "print(\"Average Baseline vs Anno Spearman: \",(baselinevsannos_spearman[0].mean()+baselinevsannos_spearman[1].mean()+baselinevsannos_spearman[2].mean())/3)\n",
    "print(\"Baseline vs AnnoMean Spearman: \",baselinevsannomean_spearman.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.77542017 0.82331933 0.80910364]\n",
      " [0.77542017 1.         0.8057423  0.77258403]\n",
      " [0.82331933 0.8057423  1.         0.85507703]\n",
      " [0.80910364 0.77258403 0.85507703 1.        ]]\n",
      "0.8068744164332401\n"
     ]
    }
   ],
   "source": [
    "num_human_anno = 4\n",
    "spearmans =np.zeros([num_human_anno,num_human_anno,length])\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    for k in range(num_human_anno):\n",
    "        for l in range(num_human_anno):\n",
    "            spearmans[k,l,j] = calculate_spearman_manual(annoscore[k,j,:],annoscore[l,j,:])\n",
    "average_spearmans = spearmans.mean(axis=2)\n",
    "\n",
    "print(average_spearmans)\n",
    "print((average_spearmans.sum()-num_human_anno)/(num_human_anno*(num_human_anno-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
