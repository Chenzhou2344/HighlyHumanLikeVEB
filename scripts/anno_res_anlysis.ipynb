{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# anno_1 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "# anno_2 = '../Human_anno/humananno_res/lsy_scene.xlsx'\n",
    "\n",
    "# anno_1oc =pd.read_excel(anno_1, usecols=['Model','scene'])\n",
    "# anno_2oc =pd.read_excel(anno_2, usecols=['Model','scene'])\n",
    "# anno_1oc = anno_1oc.astype(object).where(pd.notnull(anno_1oc), None)\n",
    "# anno_2oc = anno_2oc.astype(object).where(pd.notnull(anno_2oc), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'overall_consistency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "with open(history,'r') as f:\n",
    "    gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gptvsanno1_spearman = 0\n",
    "# gptvsanno2_spearman = 0\n",
    "# anno1vsanno2_spearman = 0\n",
    "# gptvsannomean = 0\n",
    "# badeval = []\n",
    "\n",
    "# for i in range(len(oc)):\n",
    "#     if i % 3 == 0:\n",
    "#         gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 = human_anno[:,0]\n",
    "#         anno2 = human_anno[:,1]\n",
    "#         annomean = (anno1 + anno2)/2\n",
    "#     else:\n",
    "#         gpt4o_eval_rs+= np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "#         human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "#         anno1 += human_anno[:,0]\n",
    "#         anno2 += human_anno[:,1]\n",
    "#         annomean += (anno1 + anno2)/2\n",
    "\n",
    "#     if i % 3 == 2:\n",
    "#         gptvsanno1_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "#         gptvsanno2_spearman += calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "#         anno1vsanno2_spearman += calculate_spearman_manual(anno1,anno2)\n",
    "#         gptvsannomean += calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "\n",
    "# gptvsanno1_spearman = 3*gptvsanno1_spearman/len(oc)\n",
    "# gptvsanno2_spearman = 3*gptvsanno2_spearman/len(oc)\n",
    "# anno1vsanno2_spearman = 3*  anno1vsanno2_spearman/len(oc)\n",
    "# gptvsannomean = 3*gptvsannomean/len(oc)\n",
    "# print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman)\n",
    "# print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman)\n",
    "# print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman)\n",
    "# print(\"GPT vs AnnoMean Spearman: \",gptvsannomean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT vs Anno1 Spearman:  0.5096920289855071\n",
      "GPT vs Anno2 Spearman:  0.661141304347826\n",
      "Anno1 vs Anno2 Spearman:  0.5927536231884057\n",
      "GPT vs AnnoMean Spearman:  0.6304347826086957\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gptvsbaseline_spearman' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnno1 vs Anno2 Spearman: \u001b[39m\u001b[38;5;124m\"\u001b[39m,anno1vsanno2_spearman\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT vs AnnoMean Spearman: \u001b[39m\u001b[38;5;124m\"\u001b[39m,gptvsannomean_spearman\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT vs Baseline Spearman: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mgptvsbaseline_spearman\u001b[49m\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT average score: \u001b[39m\u001b[38;5;124m\"\u001b[39m,gptscore)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnno1 average score: \u001b[39m\u001b[38;5;124m\"\u001b[39m,anno1score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gptvsbaseline_spearman' is not defined"
     ]
    }
   ],
   "source": [
    "gptvsanno1_spearman = np.zeros(len(oc))\n",
    "gptvsanno2_spearman = np.zeros(len(oc))\n",
    "anno1vsanno2_spearman = np.zeros(len(oc))\n",
    "gptvsannomean_spearman = np.zeros(len(oc))\n",
    "baselinevsanno1_spearman = np.zeros(len(oc))\n",
    "baselinevsanno2_spearman = np.zeros(len(oc))\n",
    "baselinevsannomean_spearman = np.zeros(len(oc))\n",
    "\n",
    "gptscore = np.zeros([5])\n",
    "anno1score = np.zeros([5])\n",
    "anno2score = np.zeros([5])\n",
    "annomeanscore = np.zeros([5])\n",
    "baseline_rank = np.zeros([5])\n",
    "badeval = []\n",
    "\n",
    "for i in range(len(oc)):\n",
    "    gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values())[2:])\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values())[2:])\n",
    "    baseline_rank = np.array(list(oc[i]['baseline_rank'].values())[2:])\n",
    "\n",
    "    anno1 = human_anno[:,0]\n",
    "    anno2 = human_anno[:,1]\n",
    "    annomean = (anno1 + anno2)/2\n",
    "\n",
    "\n",
    "    gptscore += gpt4o_eval_rs\n",
    "    anno1score += anno1\n",
    "    anno2score += anno2\n",
    "    annomeanscore += annomean\n",
    "\n",
    "\n",
    "    gptvsanno1 = calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "    gptvsanno2 = calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "    anno1vsanno2 = calculate_spearman_manual(anno1,anno2)\n",
    "    gptvsannomean = calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "    baselinevsanno1 = calculate_spearman_manual(baseline_rank,anno1)\n",
    "    baselinevsanno2 = calculate_spearman_manual(baseline_rank,anno2)\n",
    "    baselinevsannomean = calculate_spearman_manual(baseline_rank,annomean)\n",
    "\n",
    "    gptvsanno1_spearman[i] = gptvsanno1\n",
    "    gptvsanno2_spearman[i] = gptvsanno2\n",
    "    anno1vsanno2_spearman[i] = anno1vsanno2\n",
    "    gptvsannomean_spearman[i] = gptvsannomean\n",
    "    baselinevsanno1_spearman[i] = baselinevsanno1\n",
    "    baselinevsanno2_spearman[i] = baselinevsanno2\n",
    "    baselinevsannomean_spearman[i] = baselinevsannomean\n",
    "\n",
    "    if gptvsannomean <0.3:\n",
    "        badeval.append(i)\n",
    "\n",
    "gptscore = gptscore/len(oc)\n",
    "anno1score = anno1score/len(oc)\n",
    "anno2score = anno2score/len(oc)\n",
    "annomeanscore = annomeanscore/len(oc)\n",
    "\n",
    "print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "print(\"Anno1 vs Anno2 Spearman: \",anno1vsanno2_spearman.mean())\n",
    "print(\"GPT vs AnnoMean Spearman: \",gptvsannomean_spearman.mean())\n",
    "print(\"Baseline vs Anno1 Spearman: \",baselinevsanno1_spearman.mean())\n",
    "print(\"Baseline vs Anno2 Spearman: \",baselinevsanno2_spearman.mean())\n",
    "print(\"Baseline vs AnnoMean Spearman: \",baselinevsannomean_spearman.mean())\n",
    "\n",
    "\n",
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"Anno1 average score: \",anno1score)\n",
    "print(\"Anno2 average score: \",anno2score)\n",
    "print(\"AnnoMean average score: \",annomeanscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "badeval_prompt = []\n",
    "for i in badeval:\n",
    "    # print(\"spearman\",gptvsannomean_spearman[i])\n",
    "# print(\"videos\",oc[i]['videos']['gen2'])\n",
    "    # print(\"prompt\",oc[i]['prompt_en'])\n",
    "    badeval_prompt.append(oc[i]['prompt_en'])\n",
    "    # print(\"GPT score\",oc[i]['gpt4o_eval'])\n",
    "    # print(\"Anno score\",oc[i]['human_anno'])\n",
    "    # print(\"GPT reasons\",gpt4o_eval_history[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conuter = Counter(badeval_prompt)\n",
    "repeated = {k:v for k,v in conuter.items() if v>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(repeated.values()))\n",
    "# print(repeated)\n",
    "# with open('D:\\Astudying\\VideoEval\\HighlyHumanLikeVEB\\Human_anno\\BadEval4OverallConsistency.json','w') as f:\n",
    "#     json.dump(repeated,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.175,  0.825,  0.85 ,  0.35 ,  0.825,  0.775,  0.925,  0.85 ,\n",
       "        0.8  ,  0.9  ,  0.3  ,  0.9  ,  0.775,  0.775,  0.75 ,  0.95 ,\n",
       "        0.675,  0.625,  0.7  ,  0.925,  0.825,  0.45 ,  0.9  ,  0.775,\n",
       "        0.625,  0.45 ,  0.625,  0.325,  0.6  ,  0.8  ,  0.4  ,  0.075,\n",
       "        0.275,  0.2  ,  0.175,  0.525,  0.325, -0.175,  0.8  , -0.4  ,\n",
       "       -0.425, -0.15 ,  0.075,  0.   , -0.575, -0.075, -0.25 ,  0.   ,\n",
       "       -0.3  , -0.2  ,  0.5  ,  0.825,  0.025,  0.025,  0.4  ,  0.8  ,\n",
       "       -0.075,  1.   ,  0.7  ,  0.975,  0.5  ,  0.45 ,  0.275,  0.225,\n",
       "        0.775,  0.4  ,  0.525,  0.4  ,  0.725,  0.825,  0.825,  0.525,\n",
       "        0.8  ,  0.425,  0.8  ,  0.425,  0.25 ,  0.35 ,  0.4  ,  0.6  ,\n",
       "        0.425,  0.8  ,  0.9  ,  0.725,  0.775,  0.475,  0.525,  0.825,\n",
       "        0.775,  0.775,  0.825,  0.825,  0.45 ,  0.675,  0.8  , -0.075,\n",
       "        0.925, -0.55 ,  0.175,  0.85 ,  0.95 ,  0.45 ,  0.825, -0.225,\n",
       "        0.425,  0.   ,  0.675, -0.625,  0.55 ,  0.95 , -0.1  ,  0.05 ,\n",
       "        0.975,  0.275,  0.425,  0.675,  0.65 ,  0.125, -0.075,  0.225,\n",
       "        0.425,  1.   ,  0.775,  0.125, -0.4  , -0.25 ,  0.15 ,  0.475,\n",
       "        0.925,  0.925,  0.125,  0.625,  0.725,  0.55 ,  0.4  ,  0.7  ,\n",
       "        0.775,  0.475,  0.675,  0.65 ,  0.5  ,  0.95 ,  0.9  ,  0.525,\n",
       "        0.8  ,  0.9  ,  0.875,  0.9  ,  0.8  ,  0.8  ,  0.75 ,  0.9  ,\n",
       "        1.   ,  0.275,  1.   ,  0.9  ,  0.225,  0.3  ,  0.125,  0.325,\n",
       "        0.75 ,  0.775,  0.675,  0.8  ,  0.525,  0.975,  0.35 ,  0.575,\n",
       "        0.975,  0.675,  0.725,  0.925,  0.9  ,  0.925,  0.675,  0.425,\n",
       "        0.5  , -0.725, -0.425,  1.   ,  0.775,  0.525,  0.15 ,  0.025,\n",
       "        0.675,  0.625,  0.   ,  0.275,  0.45 ,  0.85 ,  0.425,  0.5  ,\n",
       "        0.25 ,  0.9  ,  0.525,  0.675,  0.3  ,  0.525,  0.625,  0.85 ,\n",
       "        0.85 , -0.6  ,  0.05 ,  0.5  ,  0.275, -0.275,  0.675,  0.05 ,\n",
       "        0.375,  1.   , -0.225, -0.075,  0.775, -0.55 ,  0.525,  0.075,\n",
       "        0.925,  0.4  ,  0.45 ,  0.75 ,  0.825,  0.175,  0.825,  0.525,\n",
       "        0.675,  0.925,  0.875,  0.875,  0.425,  0.775,  0.775,  0.775,\n",
       "        0.725,  0.85 ,  0.   ,  0.175,  0.325,  0.975,  0.925,  0.25 ,\n",
       "        0.775,  0.975,  0.45 ,  0.775,  0.55 ,  0.675,  0.425,  0.525,\n",
       "        0.525,  0.625,  0.775,  1.   ,  0.8  ,  0.55 , -0.075,  0.   ,\n",
       "        0.825,  0.55 ,  0.075,  0.825,  0.075,  0.925,  0.775, -0.475,\n",
       "        0.4  ,  0.725,  0.775,  0.925,  0.975,  0.8  ,  0.9  ,  0.875,\n",
       "        0.125,  0.675,  0.   ,  0.675])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptvsanno1_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anno_1oc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43manno_1oc\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'anno_1oc' is not defined"
     ]
    }
   ],
   "source": [
    "anno_1oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
