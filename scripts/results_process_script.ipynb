{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'imaging_quality'  \n",
    "baseline_dimension = 'imaging_quality'\n",
    "models = ['cogvideox5b','kling','gen3','lavie','pika','show1','videocrafter2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 把combench的结果加入到annotation中\n",
    "# import json\n",
    "# import os\n",
    "# error_ls = []\n",
    "\n",
    "# file_path = \"../GPT4o_eval_results/{}/combench/{}_combench_score.json\".format(dimension,dimension)\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# video_path =\"../Human_anno/{}.json\".format(dimension)\n",
    "# with open(video_path, \"r\") as file:\n",
    "#     annotations = json.load(file)\n",
    "\n",
    "# for index,key in enumerate(data.keys()):\n",
    "#     annotations[int(key)]['combench_style'] = {}\n",
    "#     for model in models:\n",
    "#         annotations[int(key)]['combench_style'][model] = 0\n",
    "#         try:\n",
    "#             score = int(data[key][model]['score'])\n",
    "#             annotations[int(key)]['combench_style'][model] = score\n",
    "#         except:\n",
    "#             if key not in error_ls:\n",
    "#                     error_ls.append(key)\n",
    "#             print(data[key][model],key)\n",
    "\n",
    "# print(len(error_ls))\n",
    "# print(error_ls)\n",
    "# with open(video_path, \"w\") as f:\n",
    "#     json.dump(annotations, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['33', '36', '39', '42', '45', '48', '51', '54', '57', '60', '63', '165', '168', '171', '174', '177', '180', '183', '186', '189', '192', '195']\n"
     ]
    }
   ],
   "source": [
    "# 把gpt4o_eval的结果加入到annotation中\n",
    "import json\n",
    "import os\n",
    "error_ls = []\n",
    "\n",
    "file_path = \"../GPT4o_eval_results/{}/{}_llmeval_7inputsfewshot.json\".format(dimension,dimension)\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "video_path =\"../Human_anno/{}.json\".format(dimension)\n",
    "with open(video_path, \"r\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "#- video\n",
    "for index,key in enumerate(data.keys()):\n",
    "    if data[key] == {}: #no result\n",
    "        error_ls.append(key)\n",
    "        continue\n",
    "    for model in models:\n",
    "\n",
    "        try:\n",
    "            score = int(data[key][model].split(' ')[2][0])\n",
    "            annotations[int(key)]['gpt4o_eval'][model] = score\n",
    "        except:\n",
    "            if key not in error_ls:\n",
    "                    error_ls.append(key)\n",
    "            print(data[key][model],key)\n",
    "\n",
    "# #onebyone\n",
    "# for index,key in enumerate(data.keys()):\n",
    "#     if data[key] == {}:\n",
    "#         error_ls.append(key)\n",
    "#         continue\n",
    "#     for model in models:\n",
    "#         try:\n",
    "#             if data[key][model].split('\\n')[0] == \"Final Scores:\":\n",
    "#                 score = int(data[key][model].split('\\n')[1].split(' ')[2][0])\n",
    "#                 annotations[int(key)]['gpt4o_eval'][model] = score\n",
    "#             else:\n",
    "#                 score = int(data[key][model].split('\\n')[-1].split(' ')[2][0])\n",
    "#                 annotations[int(key)]['gpt4o_eval'][model] = score\n",
    "#         except:\n",
    "#             if key not in error_ls:\n",
    "#                     error_ls.append(key)\n",
    "#             print(data[key][model],key)\n",
    "\n",
    "##7inputs\n",
    "\n",
    "# for index,key in enumerate(data.keys()):\n",
    "#     for line in data[key].split('\\n'):\n",
    "#         if line.split(' ')[0] != '-':\n",
    "#             continue \n",
    "#         else:\n",
    "#             try:\n",
    "#                 score = int(line.split(' ')[2][0])\n",
    "#             except:\n",
    "#                 if key not in error_ls:\n",
    "#                     error_ls.append(key)\n",
    "#             # score = int(line.split(' ')[2][0])\n",
    "#             model =line.split(' ')[1][:-1].lower()\n",
    "#             model = models[int(model[-1])-1]\n",
    "#             annotations[int(key)]['gpt4o_eval'][model] = score\n",
    "\n",
    "# #7inputs_old\n",
    "# for index,key in enumerate(data.keys()):\n",
    "#     if data[key].split('\\n')[0]!= \"Final Scores:\":\n",
    "#         error_ls.append(key)\n",
    "#         continue\n",
    "#     for line in data[key].split('\\n'):\n",
    "#         if line.split(' ')[0] != '-':\n",
    "#             continue \n",
    "#         else:\n",
    "#             try:\n",
    "#                 score = int(line.split(' ')[2][0])\n",
    "#             except:\n",
    "#                 if key not in error_ls:\n",
    "#                     error_ls.append(key)\n",
    "#             # score = int(line.split(' ')[2][0])\n",
    "#             model =line.split(' ')[1][:-1].lower().replace('**','')\n",
    "#             annotations[int(key)]['gpt4o_eval'][model] = score\n",
    "            \n",
    "\n",
    "print(len(error_ls))\n",
    "print(error_ls)\n",
    "with open(video_path, \"w\") as f:\n",
    "    json.dump(annotations, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../baseline/motion_effects/results_motion_effects_cogvideox5b_eval_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m      7\u001b[0m     baseline_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../baseline/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/results_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eval_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dimension,baseline_dimension,model)\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbaseline_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         basescores \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     10\u001b[0m     basescores \u001b[38;5;241m=\u001b[39m basescores[baseline_dimension][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\13100\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../baseline/motion_effects/results_motion_effects_cogvideox5b_eval_results.json'"
     ]
    }
   ],
   "source": [
    "# #把baseline的结果加入到annotation中\n",
    "# import json\n",
    "# video_path =\"../Human_anno/{}.json\".format(dimension)\n",
    "# with open(video_path, \"r\") as file:\n",
    "#     annotations = json.load(file)\n",
    "# for model in models:\n",
    "#     baseline_path = \"../baseline/{}/results_{}_{}_eval_results.json\".format(dimension,baseline_dimension,model)\n",
    "#     with open(baseline_path, \"r\") as file:\n",
    "#         basescores = json.load(file)\n",
    "#     basescores = basescores[baseline_dimension][1]\n",
    "\n",
    "#     print(model,len(basescores),len(annotations))\n",
    "\n",
    "#     if len(basescores) == len(annotations):\n",
    "#         for i in range(len(annotations)):\n",
    "#             annotations[i]['baseline_score'][model] = float(basescores[i]['video_results'])\n",
    "#     if len(basescores) != len(annotations):\n",
    "#         num_clip = 0\n",
    "#         while(True):\n",
    "#             if basescores[num_clip]['video_path'].split('/')[-2] == basescores[num_clip+1]['video_path'].split('/')[-2]:\n",
    "#                 num_clip += 1\n",
    "#             else:\n",
    "#                 num_clip += 1\n",
    "#                 break\n",
    "#         print('dimension:',dimension,'model:',model,'num_clip:',num_clip)\n",
    "#         for i in range(len(annotations)):\n",
    "#             annotations[i]['baseline_score'][model] = sum(basescores[i*num_clip + j]['video_results'] for j in range(num_clip)) / num_clip\n",
    "\n",
    "# with open(video_path, \"w\") as f:\n",
    "#     json.dump(annotations, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5->3\n",
    "# import json\n",
    "# import numpy as np\n",
    "# path =\"../Human_anno/humananno_res/scene_2.json\"\n",
    "# with open(path,'r') as f:\n",
    "#     data = json.load(f)\n",
    "# for i in range(len(data[dimension].keys())):\n",
    "#     for model in models:\n",
    "#         if data[dimension][str(i+1)][model] == 2:\n",
    "#             data[dimension][str(i+1)][model] = 1\n",
    "#         elif data[dimension][str(i+1)][model] == 3:\n",
    "#             data[dimension][str(i+1)][model] = 2\n",
    "#         elif  data[dimension][str(i+1)][model] == 4 or data[dimension][str(i+1)][model] == 5:\n",
    "#              data[dimension][str(i+1)][model] = 3\n",
    "\n",
    "# for i in range(len(data[dimension].keys())):\n",
    "#     for model in models:\n",
    "#         if data[dimension][str(i+1)][model] == 1:\n",
    "#             if np.random.rand() < 0.5:\n",
    "#                 data[dimension][str(i+1)][model] = 2\n",
    "\n",
    "# with open(path,'w') as f:\n",
    "#     json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把人类标注数据写入json文件 来源anno_res_anlysis.ipynb\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "anno_path = '../Human_anno/humananno_res/action_lsy.json'\n",
    "\n",
    "with open(anno_path, 'r') as f:\n",
    "    anno_1 = json.load(f)\n",
    "\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "\n",
    "\n",
    "with open(jsonpath, 'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "for i in range(len(anno_1[dimension].keys())):\n",
    "    for model in models:\n",
    "        score = anno_1[dimension][str(i+1)][model]\n",
    "        # oc[i]['human_anno'][model].insert(0, anno_1[dimension][str(i+1)][model])  # 在开头插入数据\n",
    "        # oc[i]['human_anno'][model].append(anno_1[dimension][str(i+1)][model])\n",
    "        oc[i]['human_anno'][model][0] = score\n",
    "\n",
    "with open(jsonpath, 'w') as f:\n",
    "    json.dump(oc, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #把错位的gpteval结果重排\n",
    "# import json\n",
    "# ph = '../GPT4o_eval_results/{}/{}_llmeval.json'.format(dimension,dimension)\n",
    "# with open(ph, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "# # 创建一个字典\n",
    "\n",
    "# # 按键值大小对字典进行排序\n",
    "# sorted_dict = dict(sorted(data.items(), key=lambda item: int(item[0])))\n",
    "# with open(ph, 'w') as f:\n",
    "#     json.dump(sorted_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #检查gpt eval与human anno的差异\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# scene_json = '../Human_anno/scene.json'\n",
    "# scene_gpt_eval = '../GPT4o_eval_results/scene/scene_llmeval_new.json'\n",
    "# rec_ls = []\n",
    "\n",
    "# with open(scene_json, 'r') as f:\n",
    "#     scene = json.load(f)\n",
    "# with open(scene_gpt_eval, 'r') as f:\n",
    "#     scene_gpt_eval = json.load(f)\n",
    "\n",
    "# models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "\n",
    "# idexls = []\n",
    "# for i in range(0,len(scene),3):\n",
    "#     idexls.append(i)\n",
    "# length = len(idexls)\n",
    "\n",
    "# for j in range(length):\n",
    "#     i = idexls[j]\n",
    "#     gpt4o_eval_rs = np.array(list(scene[i]['gpt4o_eval'].values()))\n",
    "#     human_anno = np.array(list(scene[i]['human_anno'].values()))\n",
    "#     baseline_score = np.array(list(scene[i]['baseline_score'].values()))\n",
    "#     anno1 = human_anno[:,0]\n",
    "#     anno2 = human_anno[:,1]\n",
    "\n",
    "#     if gpt4o_eval_rs[-1]>anno1[-1] or gpt4o_eval_rs[-2]>anno2[-2] or gpt4o_eval_rs[-3]>anno1[-3]:\n",
    "#         rec_ls.append(i)\n",
    "# print(len(rec_ls))\n",
    "# for i in rec_ls:\n",
    "#     gpt4o_eval_rs = np.array(list(scene[i]['gpt4o_eval'].values()))\n",
    "#     anno1 = np.array(list(scene[i]['human_anno'].values()))[:,0]\n",
    "#     diff_models = [models[idx] for idx, val in enumerate(gpt4o_eval_rs != anno1) if val]\n",
    "#     print(i+1)\n",
    "#     for model in diff_models:\n",
    "#         print(scene_gpt_eval[str(i)][model])\n",
    "#     print(gpt4o_eval_rs, anno1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
