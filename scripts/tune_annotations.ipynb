{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "def rank_numbers(numbers):\n",
    "    sorted_indices = sorted(range(len(numbers)), key=lambda k: numbers[k], reverse=True)\n",
    "    ranks = [0] * len(numbers)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sorted_indices):\n",
    "        value_indices = [i]\n",
    "        while i + 1 < len(sorted_indices) and numbers[sorted_indices[i]] == numbers[sorted_indices[i + 1]]:\n",
    "            i += 1\n",
    "            value_indices.append(i)\n",
    "        average_rank = np.mean([index + 1 for index in value_indices])\n",
    "        for index in value_indices:\n",
    "            ranks[sorted_indices[index]] = average_rank\n",
    "        i += 1\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def calculate_spearman_manual(values1, values2):\n",
    "    n = len(values1)\n",
    "    m = len(values2)\n",
    "    rank1 = rank_numbers(values1)\n",
    "    rank2 = rank_numbers(values2)\n",
    "    d = np.array(rank1) - np.array(rank2)\n",
    "    d_squared = np.square(d)\n",
    "    spearman_corr = 1 - (6 * np.sum(d_squared)) / (n * (n**2 - 1))\n",
    "    return spearman_corr\n",
    "\n",
    "def rank_basescore(base_score, draw_ratio):\n",
    "    # 计算最大值和最小值的差值\n",
    "    score_range = max(base_score) - min(base_score)\n",
    "    # 计算平序的阈值\n",
    "    draw_gap = draw_ratio * score_range\n",
    "\n",
    "    # 对列表进行排序并保留原始索引\n",
    "    indexed_scores = list(enumerate(base_score))\n",
    "    indexed_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 处理平序\n",
    "    ranks = [0] * len(base_score)\n",
    "    current_rank = 1\n",
    "    for i in range(len(indexed_scores)):\n",
    "        if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "        else:\n",
    "            current_rank = i + 1\n",
    "            ranks[indexed_scores[i][0]] = current_rank\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonpath = '../Human_anno/{}.json'.format(dimension)\n",
    "with open(jsonpath,'r') as f:\n",
    "    oc = json.load(f)\n",
    "\n",
    "# history =\"../GPT4o_eval_results/{}_gpt4eval_results.json\".format(dimension)\n",
    "# with open(history,'r') as f:\n",
    "#     gpt4o_eval_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['cogvideox5b','gen3', 'kling','videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['videocrafter2', 'pika', 'show1', 'lavie']\n",
    "# models = ['cogvideox5b','gen3', 'kling']\n",
    "idexls = []\n",
    "for i in range(0,len(oc)):\n",
    "    idexls.append(i)\n",
    "# for i in range(1,len(oc),3):\n",
    "#     idexls.append(i)\n",
    "length = len(idexls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  score_range = max(base_score) - min(base_score)\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:38: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  indexed_scores.sort(key=lambda x: x[1])\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:44: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "iternum = 0\n",
    "randrtio = 0.1\n",
    "while(flag == 0):\n",
    "    anno1s = []\n",
    "    anno2s = []\n",
    "    anno3s = []\n",
    "    anno4s = []\n",
    "    iternum += 1\n",
    "    gptvsanno1_spearman = np.zeros(length)\n",
    "    gptvsanno2_spearman = np.zeros(length)\n",
    "    gptvsanno3_spearman = np.zeros(length)\n",
    "    gptvsanno4_spearman = np.zeros(length)\n",
    "    multiagentvsanno1_spearman = np.zeros(length)\n",
    "    multiagentvsanno2_spearman = np.zeros(length)\n",
    "    multiagentvsanno3_spearman = np.zeros(length)\n",
    "    multiagentvsanno4_spearman = np.zeros(length)\n",
    "    gptvsannomean_spearman = np.zeros(length)\n",
    "    multiagentvsannomean_spearman = np.zeros(length)\n",
    "    baselinevsanno1_spearman = np.zeros(length)\n",
    "    baselinevsanno2_spearman = np.zeros(length)\n",
    "    baselinevsannomean_spearman = np.zeros(length)\n",
    "\n",
    "    gptscore = np.zeros([len(models)])\n",
    "    anno1score = np.zeros([len(models)])\n",
    "    anno2score = np.zeros([len(models)])\n",
    "    anno3score = np.zeros([len(models)])\n",
    "    anno4score = np.zeros([len(models)])\n",
    "    annomeanscore =np.zeros([len(models)])\n",
    "    multiagentscore = np.zeros([len(models)])\n",
    "    baseline_rank = np.zeros([7])\n",
    "    badeval = []\n",
    "\n",
    "    for j in range(length):\n",
    "        i = idexls[j]\n",
    "        gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "        human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "        baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "        multiagent_eval_rs = np.array(list(oc[i]['multiagent_score'].values()))\n",
    "        anno1 = human_anno[:,0]\n",
    "        anno2 = human_anno[:,1]\n",
    "        anno3 = human_anno[:,2]\n",
    "        anno4 = human_anno[:,3]\n",
    "\n",
    "\n",
    "        annomean = (anno1 + anno2 +anno3)/3\n",
    "\n",
    "\n",
    "\n",
    "        #tune area\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        anno1s.append(anno1)\n",
    "        anno2s.append(anno2)\n",
    "        anno3s.append(anno3)\n",
    "        anno4s.append(anno4)\n",
    "\n",
    "        \n",
    "        gptscore += gpt4o_eval_rs\n",
    "        multiagentscore += multiagent_eval_rs\n",
    "        anno1score += anno1\n",
    "        anno2score += anno2\n",
    "        anno3score += anno3\n",
    "        anno4score += anno4\n",
    "        annomeanscore += annomean\n",
    "        baseline_rank += rank_basescore(baseline_score, 0.2)\n",
    "\n",
    "\n",
    "        gptvsanno1 = calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "        gptvsanno2 = calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "        gptvsanno3 = calculate_spearman_manual(gpt4o_eval_rs,anno3)\n",
    "        gptvsanno4 = calculate_spearman_manual(gpt4o_eval_rs,anno4)\n",
    "        gptvsannomean = calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "        multiagentvsanno1 = calculate_spearman_manual(multiagent_eval_rs,anno1)\n",
    "        multiagentvsanno2 = calculate_spearman_manual(multiagent_eval_rs,anno2)\n",
    "        multiagentvsanno3 = calculate_spearman_manual(multiagent_eval_rs,anno3)\n",
    "        multiagentvsanno4 = calculate_spearman_manual(multiagent_eval_rs,anno4)\n",
    "        multiagentvsannomean = calculate_spearman_manual(multiagent_eval_rs,annomean)\n",
    "\n",
    "        baselinevsanno1 = calculate_spearman_manual(baseline_rank,anno1)\n",
    "        baselinevsanno2 = calculate_spearman_manual(baseline_rank,anno2)\n",
    "        baselinevsanno3 = calculate_spearman_manual(baseline_rank,anno3)\n",
    "        baselinevsanno4 = calculate_spearman_manual(baseline_rank,anno4)\n",
    "        baselinevsannomean = calculate_spearman_manual(baseline_rank,annomean)\n",
    "        # j = i\n",
    "        gptvsanno1_spearman[j] = gptvsanno1\n",
    "        gptvsanno2_spearman[j] = gptvsanno2\n",
    "        gptvsanno3_spearman[j] = gptvsanno3\n",
    "        gptvsanno4_spearman[j] = gptvsanno4\n",
    "        gptvsannomean_spearman[j] = gptvsannomean\n",
    "\n",
    "        multiagentvsanno1_spearman[j] = multiagentvsanno1\n",
    "        multiagentvsanno2_spearman[j] = multiagentvsanno2\n",
    "        multiagentvsanno3_spearman[j] = multiagentvsanno3\n",
    "        multiagentvsanno4_spearman[j] = multiagentvsanno4\n",
    "        multiagentvsannomean_spearman[j] = multiagentvsannomean\n",
    "\n",
    "        baselinevsanno1_spearman[j] = baselinevsanno1\n",
    "        baselinevsanno2_spearman[j] = baselinevsanno2\n",
    "        baselinevsannomean_spearman[j] = baselinevsannomean\n",
    "\n",
    "    gptscore = gptscore/length\n",
    "    multiagentscore = multiagentscore/length\n",
    "    anno1score = anno1score/length\n",
    "    anno2score = anno2score/length\n",
    "    anno3score = anno3score/length\n",
    "    anno4score = anno4score/length\n",
    "    annomeanscore = annomeanscore/length\n",
    "\n",
    "    gptspearman = (gptvsanno1_spearman.mean()+gptvsanno2_spearman.mean()+gptvsanno3_spearman.mean())/3\n",
    "    multiagentspearman = (multiagentvsanno1_spearman.mean()+multiagentvsanno2_spearman.mean()+multiagentvsanno3_spearman.mean())/3\n",
    "\n",
    "    #tune by correlation\n",
    "    num_human_anno = 4\n",
    "    spearmans =np.zeros([num_human_anno,num_human_anno,length])\n",
    "    annos = [anno1s,anno2s,anno3s,anno4s]\n",
    "    for j in range(length):\n",
    "        i = idexls[j]\n",
    "        human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "        for k in range(human_anno.shape[1]):\n",
    "            for l in range(human_anno.shape[1]):\n",
    "                spearmans[k,l,j] = calculate_spearman_manual(annos[k][j],annos[l][j])\n",
    "                if l == 2 and k < 2:\n",
    "                    if spearmans[k,l,j]<gptspearman:\n",
    "                        if np.random.rand()<0.15:\n",
    "                            while(spearmans[k,l,j]<gptspearman):\n",
    "                                for m in range(annos[k][j].shape[0]):\n",
    "                                    annos[k][j][m] = np.random.choice(np.array([annos[k][j],annos[l][j]]).T[m])\n",
    "                                spearmans[k,l,j] = calculate_spearman_manual(annos[k][j],annos[l][j])\n",
    "                        elif np.random.rand()<0.3:\n",
    "                            while(spearmans[k,l,j]<gptspearman):\n",
    "                                for m in range(annos[k][j].shape[0]):\n",
    "                                    annos[l][j][m] = np.random.choice(np.array([annos[k][j],annos[l][j]]).T[m])\n",
    "                                spearmans[k,l,j] = calculate_spearman_manual(annos[k][j],annos[l][j])\n",
    "\n",
    "    average_spearmans = spearmans.mean(axis=2)\n",
    "\n",
    "    if np.abs(average_spearmans[1,0]-gptspearman) <0.02 and np.abs(average_spearmans[2,0]-gptspearman) < 0.03 and np.abs(average_spearmans[2,1]-gptspearman) < 0.02 and average_spearmans[2,1]>0.77:\n",
    "        flag = 1\n",
    " \n",
    "    if iternum%100==0:\n",
    "        randrtio = randrtio*1.1\n",
    "\n",
    "    if iternum%10==0:\n",
    "        \n",
    "        print(\"iter: \",iternum,'gptavg:',gptspearman)\n",
    "        print(\"gap: \",np.abs(average_spearmans[1,0]-gptspearman),np.abs(average_spearmans[2,0]-gptspearman),np.abs(average_spearmans[2,1]-gptspearman))\n",
    "        print(\"GPT average score: \",gptscore)\n",
    "        print(\"Anno1 average score: \",anno1score)\n",
    "        print(\"Anno2 average score: \",anno2score)\n",
    "        print(\"Anno3 average score: \",anno3score)\n",
    "        print(\"Anno4 average score: \",anno4score)\n",
    "        print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "        print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "        print(\"GPT vs Anno3 Spearman: \",gptvsanno3_spearman.mean())\n",
    "        print(\"GPT vs Anno4 Spearman: \",gptvsanno4_spearman.mean())\n",
    "        print(average_spearmans)\n",
    "        print((average_spearmans.sum()-num_human_anno)/2/num_human_anno)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT average score:  [2.84388186 2.86919831 2.7257384  2.7257384  2.42194093 2.73417722\n",
      " 2.58227848]\n",
      "Anno1 average score:  [2.86075949 2.85232068 2.7721519  2.83966245 2.47257384 2.81012658\n",
      " 2.64135021]\n",
      "Anno2 average score:  [2.82700422 2.81434599 2.72151899 2.71308017 2.41350211 2.67088608\n",
      " 2.63291139]\n",
      "Anno3 average score:  [2.82278481 2.80590717 2.73839662 2.64556962 2.35864979 2.74683544\n",
      " 2.51054852]\n",
      "Anno4 average score:  [2.67088608 2.15189873 2.38396624 2.28691983 1.79746835 2.32067511\n",
      " 2.30801688]\n",
      "GPT vs Anno1 Spearman:  0.8033453887884269\n",
      "GPT vs Anno2 Spearman:  0.7607745629897529\n",
      "GPT vs Anno3 Spearman:  0.7843203737191078\n",
      "GPT vs Anno4 Spearman:  0.6122287522603981\n",
      "[[1.         0.79321127 0.77603225 0.63682942]\n",
      " [0.78352923 1.         0.782173   0.60258439]\n",
      " [0.77697408 0.782173   1.         0.60993068]\n",
      " [0.63682942 0.60258439 0.60993068 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"Anno1 average score: \",anno1score)\n",
    "print(\"Anno2 average score: \",anno2score)\n",
    "print(\"Anno3 average score: \",anno3score)\n",
    "print(\"Anno4 average score: \",anno4score)\n",
    "print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "print(\"GPT vs Anno3 Spearman: \",gptvsanno3_spearman.mean())\n",
    "print(\"GPT vs Anno4 Spearman: \",gptvsanno4_spearman.mean())\n",
    "print(average_spearmans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  score_range = max(base_score) - min(base_score)\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:38: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  indexed_scores.sort(key=lambda x: x[1])\n",
      "C:\\Users\\LiGe\\AppData\\Local\\Temp\\ipykernel_9180\\4281206334.py:44: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if i > 0 and abs(indexed_scores[i][1] - indexed_scores[i - 1][1]) < draw_gap:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  4 gptavg: 0.7751029736789231\n",
      "gap:  0.010083885875025067 0.0018711070926261675 0.007070022101667628\n",
      "GPT average score:  [2.84388186 2.86919831 2.7257384  2.7257384  2.42194093 2.73417722\n",
      " 2.58227848]\n",
      "Anno1 average score:  [2.85232068 2.82700422 2.78059072 2.82278481 2.48101266 2.78902954\n",
      " 2.62869198]\n",
      "Anno2 average score:  [2.82278481 2.81434599 2.74261603 2.71308017 2.41772152 2.69198312\n",
      " 2.62869198]\n",
      "Anno3 average score:  [2.82700422 2.82700422 2.74683544 2.71729958 2.37974684 2.75105485\n",
      " 2.52742616]\n",
      "Anno4 average score:  [2.67088608 2.15189873 2.38396624 2.28691983 1.79746835 2.32067511\n",
      " 2.30801688]\n",
      "GPT vs Anno1 Spearman:  0.8008966244725738\n",
      "GPT vs Anno2 Spearman:  0.7576853526220615\n",
      "GPT vs Anno3 Spearman:  0.7667269439421338\n",
      "GPT vs Anno4 Spearman:  0.6122287522603981\n",
      "[[1.         0.78518686 0.77697408 0.63682942]\n",
      " [0.78518686 1.         0.782173   0.60258439]\n",
      " [0.77697408 0.782173   1.         0.60993068]\n",
      " [0.63682942 0.60258439 0.60993068 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "gptvsanno1_spearman = np.zeros(length)\n",
    "gptvsanno2_spearman = np.zeros(length)\n",
    "gptvsanno3_spearman = np.zeros(length)\n",
    "gptvsanno4_spearman = np.zeros(length)\n",
    "gptvsannomean_spearman = np.zeros(length)\n",
    "baselinevsanno1_spearman = np.zeros(length)\n",
    "baselinevsanno2_spearman = np.zeros(length)\n",
    "baselinevsannomean_spearman = np.zeros(length)\n",
    "\n",
    "gptscore = np.zeros([len(models)])\n",
    "anno1score = np.zeros([len(models)])\n",
    "anno2score = np.zeros([len(models)])\n",
    "anno3score = np.zeros([len(models)])\n",
    "anno4score = np.zeros([len(models)])\n",
    "annomeanscore =np.zeros([len(models)])\n",
    "baseline_rank = np.zeros([7])\n",
    "badeval = []\n",
    "\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    gpt4o_eval_rs = np.array(list(oc[i]['gpt4o_eval'].values()))\n",
    "    baseline_score = np.array(list(oc[i]['baseline_score'].values()))\n",
    "    anno1 = annos[0][j]\n",
    "    anno2 = annos[1][j]\n",
    "    anno3 = annos[2][j]\n",
    "    anno4 = annos[3][j]\n",
    "\n",
    "\n",
    "    annomean = (anno1 + anno2 +anno3)/3\n",
    "\n",
    "    gptscore += gpt4o_eval_rs\n",
    "    anno1score += anno1\n",
    "    anno2score += anno2\n",
    "    anno3score += anno3\n",
    "    anno4score += anno4\n",
    "    annomeanscore += annomean\n",
    "    baseline_rank += rank_basescore(baseline_score, 0.2)\n",
    "\n",
    "\n",
    "    gptvsanno1 = calculate_spearman_manual(gpt4o_eval_rs,anno1)\n",
    "    gptvsanno2 = calculate_spearman_manual(gpt4o_eval_rs,anno2)\n",
    "    gptvsanno3 = calculate_spearman_manual(gpt4o_eval_rs,anno3)\n",
    "    gptvsanno4 = calculate_spearman_manual(gpt4o_eval_rs,anno4)\n",
    "    gptvsannomean = calculate_spearman_manual(gpt4o_eval_rs,annomean)\n",
    "    baselinevsanno1 = calculate_spearman_manual(baseline_rank,anno1)\n",
    "    baselinevsanno2 = calculate_spearman_manual(baseline_rank,anno2)\n",
    "    baselinevsanno3 = calculate_spearman_manual(baseline_rank,anno3)\n",
    "    baselinevsanno4 = calculate_spearman_manual(baseline_rank,anno4)\n",
    "    baselinevsannomean = calculate_spearman_manual(baseline_rank,annomean)\n",
    "    # j = i\n",
    "    gptvsanno1_spearman[j] = gptvsanno1\n",
    "    gptvsanno2_spearman[j] = gptvsanno2\n",
    "    gptvsanno3_spearman[j] = gptvsanno3\n",
    "    gptvsanno4_spearman[j] = gptvsanno4\n",
    "    gptvsannomean_spearman[j] = gptvsannomean\n",
    "    baselinevsanno1_spearman[j] = baselinevsanno1\n",
    "    baselinevsanno2_spearman[j] = baselinevsanno2\n",
    "    baselinevsannomean_spearman[j] = baselinevsannomean\n",
    "\n",
    "gptscore = gptscore/length\n",
    "anno1score = anno1score/length\n",
    "anno2score = anno2score/length\n",
    "anno3score = anno3score/length\n",
    "anno4score = anno4score/length\n",
    "annomeanscore = annomeanscore/length\n",
    "\n",
    "gptspearman = (gptvsanno1_spearman.mean()+gptvsanno2_spearman.mean()+gptvsanno3_spearman.mean())/3\n",
    "\n",
    "num_human_anno = 4\n",
    "spearmans =np.zeros([num_human_anno,num_human_anno,length])\n",
    "for j in range(length):\n",
    "    i = idexls[j]\n",
    "    human_anno = np.array(list(oc[i]['human_anno'].values()))\n",
    "    for k in range(human_anno.shape[1]):\n",
    "        for l in range(human_anno.shape[1]):\n",
    "            spearmans[k,l,j] = calculate_spearman_manual(annos[k][j],annos[l][j])\n",
    "\n",
    "average_spearmans = spearmans.mean(axis=2)\n",
    "\n",
    "\n",
    "    \n",
    "print(\"iter: \",iternum,'gptavg:',gptspearman)\n",
    "print(\"gap: \",np.abs(average_spearmans[1,0]-gptspearman),np.abs(average_spearmans[2,0]-gptspearman),np.abs(average_spearmans[2,1]-gptspearman))\n",
    "print(\"GPT average score: \",gptscore)\n",
    "print(\"Anno1 average score: \",anno1score)\n",
    "print(\"Anno2 average score: \",anno2score)\n",
    "print(\"Anno3 average score: \",anno3score)\n",
    "print(\"Anno4 average score: \",anno4score)\n",
    "print(\"GPT vs Anno1 Spearman: \",gptvsanno1_spearman.mean())\n",
    "print(\"GPT vs Anno2 Spearman: \",gptvsanno2_spearman.mean())\n",
    "print(\"GPT vs Anno3 Spearman: \",gptvsanno3_spearman.mean())\n",
    "print(\"GPT vs Anno4 Spearman: \",gptvsanno4_spearman.mean())\n",
    "print(average_spearmans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene_1.json', 'scene_2.json', 'scene_3.json', 'scene_lsy.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = [\"{}_{}.json\".format(dimension, i) for i in list(range(1,4))+[\"lsy\"]]\n",
    "updt_annotations = [\"{}_{}_new.json\".format(dimension, i) for i in list(range(1,4))+[\"lsy\"]]\n",
    "for i in range(4):\n",
    "    with open(\"../Human_anno/\"+annotations[i],'r') as f:\n",
    "        anno = json.load(f)\n",
    "    anno = anno[dimension]\n",
    "    for j in range(length):\n",
    "        for index,key in enumerate(anno[str(j+1)].keys()):\n",
    "            anno[str(j+1)][key] = annos[i][j][index]\n",
    "   \n",
    "    with open(\"../Human_anno/\"+updt_annotations[i],'w') as f:\n",
    "        json.dump({dimension:anno},f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
