{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from tool import videoreader\n",
    "# 创建一个OpenAI客户端实例\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://gateway.ai.cloudflare.com/v1/627f1b1f372e3a198dc32573bbc6f720/openai-gpt/openai\"  # 替换为你的自定义API域\n",
    ")\n",
    "\n",
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 'temporal_consistency'\n",
    "from PromptTemplate4GPTeval import Prompt4TemperalConsistency\n",
    "prompt_template = Prompt4TemperalConsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_prepath = r'D:\\Astudying\\VideoEval\\data4dimensions'\n",
    "# data_prepath = \"../../data4dimensions/\"\n",
    "with open(\"./Human_anno/{}.json\".format(dimension)) as f:\n",
    "    human_anno = json.load(f)\n",
    "\n",
    "batch_stpath = '../batch_api/{}'.format(dimension)\n",
    "if not os.path.exists(batch_stpath):\n",
    "    os.makedirs(batch_stpath)\n",
    "\n",
    "batch_unique_ids = []\n",
    "batch_split_ids = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_batch(index_list,batch_id):\n",
    "#     requests = []\n",
    "#     for i in index_list:     \n",
    "#         request ={\"custom_id\": \"request-{}\".format(i), \n",
    "#                 \"method\": \"POST\", \n",
    "#                 \"url\": \"/v1/chat/completions\",\n",
    "#                 \"body\": {\"model\": MODEL,\n",
    "#                             \"messages\": [],\n",
    "#                             \"temperature\": 0}}\n",
    "\n",
    "#         frames = videoreader.process_video2gridview(data_prepath,human_anno[i]['videos'],4)\n",
    "\n",
    "#         prompten = human_anno[i]['prompt_en']\n",
    "#         # question = human_anno[i]['question_en']\n",
    "#         # subject = human_anno[i]['subject_en']\n",
    "#         # scene = human_anno[i]['scene_en']\n",
    "#         # objet = human_anno[i]['object']\n",
    "#         messages=[\n",
    "#         {\n",
    "#         \"role\": \"system\", \"content\":\n",
    "#             prompt_template\n",
    "#             }\n",
    "#             ,\n",
    "#         {\n",
    "#             \"role\": \"user\", \"content\": [\n",
    "            \n",
    "#             \" The following images arrange 4 key frames from 1 second video clip from videos in a 1*4 grid view.\" ,\n",
    "\n",
    "#             \"The grid view images from model1 \\n \", \n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['cogvideox5b']),\n",
    "#             \"The grid view images from model2 \\n \", \n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['kling']),\n",
    "#             \"The grid view images from model3 \\n \", \n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['gen3']),\n",
    "#             \" The grid view images from model4 \\n \",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['videocrafter2']),   \n",
    "#             \"\\nThe grid view images from model5 \\n\",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['pika']),\n",
    "#             \"\\n The grid view images from model6\\n \",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\":    f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, frames['show1']),                             \n",
    "#             \"\\nThe grid view images from  model7\\n \",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}},frames['lavie']),\n",
    "\n",
    "#            \"Assuming there are  videos scoring 'x,y,z..',provide your analysis and explanation in the output format as follows:\\n\"\n",
    "#             \"- model1: x ,because ...\\n \"\n",
    "#             \"- model2: y ,because ...\\n \"\n",
    "#             \"- model3: z ,because ...\\n \"\n",
    "#                                                         ],\n",
    " \n",
    "#             }\n",
    "#         ]\n",
    "\n",
    "#         request['body']['messages'] = messages\n",
    "\n",
    "#         requests.append(request)\n",
    "\n",
    "#     with open(os.path.join(batch_stpath,\"requests_{}_batch_{}.jsonl\".format(dimension,batch_id)), \"w\") as f:\n",
    "#         for entry in requests:\n",
    "#             json_line = json.dumps(entry)\n",
    "#             f.write(json_line + '\\n')\n",
    "    \n",
    "#     batch_input_file = client.files.create(\n",
    "#              file=open(os.path.join(batch_stpath,\"requests_{}_batch_{}.jsonl\".format(dimension,batch_id)), \"rb\"),\n",
    "#               purpose=\"batch\"\n",
    "#              )\n",
    "\n",
    "#     batch_input_file_id = batch_input_file.id    \n",
    "\n",
    "#     batch_object = client.batches.create(\n",
    "#             input_file_id=batch_input_file_id,\n",
    "#             endpoint=\"/v1/chat/completions\",\n",
    "#             completion_window=\"24h\",\n",
    "#             metadata={\n",
    "#             \"description\": \"nightly group1 {} eval job batch {}\".format(dimension,batch_id)\n",
    "#             }\n",
    "#                                         )\n",
    "\n",
    "#     batch_unique_ids.append(batch_object.id)\n",
    "#     batch_split_ids.append(batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_batch_onebyone(index_list,batch_id):\n",
    "    print(\"Thread {} is running\".format(batch_id))\n",
    "    model2message = {\n",
    "    'cogvideox5b':\"12 frames from cogvideox5b,which you need to evaluate \\n\",\n",
    "    'kling':\"10 frames from kling ,which you need to evaluate\\n \", \n",
    "    'gen3': \"10 frames from gen3 ,which you need to evaluate\\n\",\n",
    "    'videocrafter2':\"4 frames from videocrafter2,which you need to evaluate\",\n",
    "    'pika':\"7 frames from pika ,which you need to evaluate\",\n",
    "    'show1':\"8 frames from show1,which you need to evaluate \",\n",
    "    'lavie':\"5 frames from lavie ,which you need to evaluate\",\n",
    "    }\n",
    "    requests = []\n",
    "    for i in index_list:     \n",
    "        frames = videoreader.process_video2gridview(data_prepath,human_anno[i]['videos'],4)\n",
    "        for key, value in model2message.items():\n",
    "            modelname = key\n",
    "            modelmessage = value\n",
    "    \n",
    "            request ={\"custom_id\": \"request-{}-{}\".format(i,modelname), \n",
    "                    \"method\": \"POST\", \n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\"model\": MODEL,\n",
    "                                \"messages\": [],\n",
    "                                \"temperature\": 0}}\n",
    "\n",
    "            examplemodels = [x for x in model2message.keys() if x != modelname]\n",
    "\n",
    "\n",
    "            prompten = human_anno[i]['prompt_en']\n",
    "            # question = human_anno[i]['question_en']\n",
    "            # subject = human_anno[i]['subject_en']\n",
    "            # scene = human_anno[i]['scene_en']\n",
    "            # objet = human_anno[i]['object']\n",
    "            messages=[\n",
    "            {\n",
    "            \"role\": \"system\", \"content\":\n",
    "                prompt_template\n",
    "                }\n",
    "                ,\n",
    "            {\n",
    "                \"role\": \"user\", \"content\":[\n",
    "\n",
    "            \" The following images arrange the key frames of the video with one displaying the 4 frames extracted from each second in a in a 1*4 grid view.\" ,\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}},frames[modelname]),    \n",
    "\n",
    "            \"The video has some unnatural changes,texture flikering or color jitter.Try your to find  and analyse an unnatural changes,texture flikering or color jitter.\",\n",
    "            \"Then evaluate the temporal consistency of the video based on your analysis and system message.\"\n",
    "            \"Assuming there are a video scoring 'x',provide your analysis and explanation in the output format as follows:\\n\"\n",
    "            \"- video: x ,because ...\"\n",
    "              ],\n",
    "                }\n",
    "            ]\n",
    "            request['body']['messages'] = messages\n",
    "            requests.append(request)\n",
    "\n",
    "    with open(os.path.join(batch_stpath,\"requests_{}_batch_{}.jsonl\".format(dimension,batch_id)), \"w\") as f:\n",
    "        for entry in requests:\n",
    "            json_line = json.dumps(entry)\n",
    "            f.write(json_line + '\\n')\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "             file=open(os.path.join(batch_stpath,\"requests_{}_batch_{}.jsonl\".format(dimension,batch_id)), \"rb\"),\n",
    "              purpose=\"batch\"\n",
    "             )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id    \n",
    "\n",
    "    batch_object = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "            \"description\": \"nightly group1 {} eval job batch {}\".format(dimension,batch_id)\n",
    "            }\n",
    "                                        )\n",
    "    \n",
    "    batch_split_ids.append(batch_id)\n",
    "    batch_unique_ids.append(batch_object.id)\n",
    "\n",
    "    print(\"Thread {} is done\".format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(range(1,len(human_anno),3))\n",
    "# l2 = list(range(2,len(human_anno),3))\n",
    "# l3 = list(range(0,len(human_anno),3))\n",
    "\n",
    "# l = [int(x) for x in['204', '207', '210', '213', '216', '219', '222', '225', '228', '231']]\n",
    "\n",
    "\n",
    "ls = l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 is running\n",
      "Thread 1 is running\n",
      "Thread 2 is running\n",
      "Thread 3 is running\n",
      "Thread 4 is running\n",
      "Thread 5 is running\n",
      "Thread 6 is running\n",
      "Thread 7 is running\n",
      "Thread 8 is running\n",
      "Thread 9 is running\n",
      "Thread 10 is running\n",
      "Thread 11 is running\n",
      "Thread 12 is running\n",
      "Thread 13 is running\n",
      "Thread 14 is running\n",
      "Thread 15 is running\n",
      "All threads started\n",
      "Thread 15 is done\n",
      "Thread 3 is done\n",
      "Thread 6 is done\n",
      "Thread 10 is done\n",
      "Thread 8 is done\n",
      "Thread 14 is done\n",
      "Thread 9 is done\n",
      "Thread 5 is done\n",
      "Thread 0 is done\n",
      "Thread 2 is done\n",
      "Thread 1 is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\13100\\AppData\\Local\\Temp\\ipykernel_35704\\3218853174.py\", line 61, in eval_batch_onebyone\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\resources\\files.py\", line 109, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'You have exceeded your file storage quota. Organizations are limited to 100 GB of files. Please delete old files or attempt with a smaller file size.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\13100\\AppData\\Local\\Temp\\ipykernel_35704\\3218853174.py\", line 61, in eval_batch_onebyone\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\resources\\files.py\", line 109, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'You have exceeded your file storage quota. Organizations are limited to 100 GB of files. Please delete old files or attempt with a smaller file size.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\13100\\AppData\\Local\\Temp\\ipykernel_35704\\3218853174.py\", line 61, in eval_batch_onebyone\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\resources\\files.py\", line 109, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'You have exceeded your file storage quota. Organizations are limited to 100 GB of files. Please delete old files or attempt with a smaller file size.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Exception in thread Thread-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\13100\\AppData\\Local\\Temp\\ipykernel_35704\\3218853174.py\", line 61, in eval_batch_onebyone\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\resources\\files.py\", line 109, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'You have exceeded your file storage quota. Organizations are limited to 100 GB of files. Please delete old files or attempt with a smaller file size.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\13100\\AppData\\Local\\Temp\\ipykernel_35704\\3218853174.py\", line 61, in eval_batch_onebyone\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\resources\\files.py\", line 109, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\13100\\miniconda3\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'You have exceeded your file storage quota. Organizations are limited to 100 GB of files. Please delete old files or attempt with a smaller file size.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "batch_size = 5\n",
    "batches = [ls[i:i + batch_size] for i in range(0, len(ls), batch_size)]\n",
    "# with open(\"./batch_infos/batch_info_{}.json\".format(dimension), \"r\") as f:\n",
    "#     batch_info = json.load(f)\n",
    "\n",
    "# batch_split_ids = batch_info['batch_split_ids']\n",
    "# batches = batch_info['videos_in_batch']\n",
    "\n",
    "threads = []\n",
    "for i, batch in enumerate(batches):\n",
    "    thread = threading.Thread(target=eval_batch_onebyone, args=(batch, i))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "print(\"All threads started\")\n",
    "# 等待所有线程完成\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "#保存batch信息\n",
    "with open(\"./batch_infos/batch_info_{}_gridstress_group2.json\".format(dimension), \"w\") as f:\n",
    "    json.dump({\"batch_unique_ids\": batch_unique_ids, \"batch_split_ids\": batch_split_ids,\"videos_in_batch\":batches}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:batch_6731ac66591481909cea26cf388a0c53 status:completed descrepition:nightly group1 temporal_consistency eval job batch 15\n",
      "batch batch_6731ac66591481909cea26cf388a0c53 done,end index 232\n",
      "id:batch_6731acfa82048190a26193794a158675 status:completed descrepition:nightly group1 temporal_consistency eval job batch 3\n",
      "batch batch_6731acfa82048190a26193794a158675 done,end index 58\n",
      "id:batch_6731ad0be9288190b6ecb53b784d0589 status:completed descrepition:nightly group1 temporal_consistency eval job batch 6\n",
      "batch batch_6731ad0be9288190b6ecb53b784d0589 done,end index 103\n",
      "id:batch_6731ad1ceccc8190902898fe78d1d1ff status:completed descrepition:nightly group1 temporal_consistency eval job batch 10\n",
      "batch batch_6731ad1ceccc8190902898fe78d1d1ff done,end index 163\n",
      "id:batch_6731ad20c06881908e96b0f334ce1efc status:completed descrepition:nightly group1 temporal_consistency eval job batch 8\n",
      "batch batch_6731ad20c06881908e96b0f334ce1efc done,end index 133\n",
      "id:batch_6731ad21a2d88190bcc51ec053ee91da status:completed descrepition:nightly group1 temporal_consistency eval job batch 14\n",
      "batch batch_6731ad21a2d88190bcc51ec053ee91da done,end index 223\n",
      "id:batch_6731ad21c4cc81908d2bf2444f2a3471 status:completed descrepition:nightly group1 temporal_consistency eval job batch 9\n",
      "batch batch_6731ad21c4cc81908d2bf2444f2a3471 done,end index 148\n",
      "id:batch_6731ad26b77881908edb280772fa6523 status:completed descrepition:nightly group1 temporal_consistency eval job batch 5\n",
      "batch batch_6731ad26b77881908edb280772fa6523 done,end index 88\n",
      "id:batch_6731ad26ccf48190b16feab9ebb04c28 status:completed descrepition:nightly group1 temporal_consistency eval job batch 0\n",
      "batch batch_6731ad26ccf48190b16feab9ebb04c28 done,end index 13\n",
      "id:batch_6731ad2b18688190bfb8f5e1440149bb status:completed descrepition:nightly group1 temporal_consistency eval job batch 2\n",
      "batch batch_6731ad2b18688190bfb8f5e1440149bb done,end index 43\n",
      "id:batch_6731ad2c29048190b817a24837df2adb status:completed descrepition:nightly group1 temporal_consistency eval job batch 1\n",
      "batch batch_6731ad2c29048190b817a24837df2adb done,end index 28\n"
     ]
    }
   ],
   "source": [
    "with open(\"./batch_infos/batch_info_{}_gridstress_group2.json\".format(dimension), \"r\") as f:\n",
    "    batch_info = json.load(f)\n",
    "\n",
    "    \n",
    "batchids = batch_info[\"batch_unique_ids\"]\n",
    "llmeval_path = \"./GPT4o_eval_results/{}/{}_llmeval_gridview_usrstress.json\".format(dimension,dimension)\n",
    "\n",
    "with open(llmeval_path, \"r\") as f:\n",
    "    llmeval = json.load(f)\n",
    "    \n",
    "for i in ls:\n",
    "    # if str(i) not in llmeval.keys():\n",
    "         llmeval[str(i)] = {}\n",
    "\n",
    "for id in batchids:\n",
    "    batch_object = client.batches.retrieve(id)\n",
    "    print(\"id:{} status:{} descrepition:{}\".format(id,batch_object.status,batch_object.metadata['description']))\n",
    "\n",
    "    if batch_object.status != \"completed\":\n",
    "        print(\"batch {} is not completed\".format(id))\n",
    "        continue    \n",
    "\n",
    "    file_response = client.files.content(batch_object.output_file_id)\n",
    "    for line in file_response.text.splitlines():\n",
    "        index = json.loads(line)[\"custom_id\"].split(\"-\")[-2]\n",
    "        model = json.loads(line)[\"custom_id\"].split(\"-\")[-1]\n",
    "\n",
    "        # index = json.loads(line)[\"custom_id\"].split(\"-\")[-1]\n",
    "\n",
    "        eval_res = json.loads(line)[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].replace('\\n\\n','\\n')\n",
    "        \n",
    "        llmeval[index][model] = eval_res\n",
    "        # llmeval[index] = eval_res\n",
    "    with open(llmeval_path, \"w\") as f:\n",
    "        json.dump(llmeval, f, indent=4)\n",
    "\n",
    "    print(\"batch {} done,end index {}\".format(id,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
